{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "258df23c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\krish'\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5ee45d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          tweet_text cyberbullying_type\n",
      "0  In other words #katandandre, your food was cra...  not_cyberbullying\n",
      "1  Why is #aussietv so white? #MKR #theblock #ImA...  not_cyberbullying\n",
      "2  @XochitlSuckkks a classy whore? Or more red ve...  not_cyberbullying\n",
      "3  @Jason_Gio meh. :P  thanks for the heads up, b...  not_cyberbullying\n",
      "4  @RudhoeEnglish This is an ISIS account pretend...  not_cyberbullying\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset (after you download from Kaggle)\n",
    "df = pd.read_csv(r'Q:\\cyberbulyying\\cyberbullying_tweets.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdaa5929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          tweet_text  \\\n",
      "0  In other words #katandandre, your food was cra...   \n",
      "1  Why is #aussietv so white? #MKR #theblock #ImA...   \n",
      "2  @XochitlSuckkks a classy whore? Or more red ve...   \n",
      "3  @Jason_Gio meh. :P  thanks for the heads up, b...   \n",
      "4  @RudhoeEnglish This is an ISIS account pretend...   \n",
      "\n",
      "                                          clean_text  \n",
      "0                            words food crapilicious  \n",
      "1                                              white  \n",
      "2                   classy whore red velvet cupcakes  \n",
      "3  meh p thanks heads concerned another angry dud...  \n",
      "4  isis account pretending kurdish account like i...  \n"
     ]
    }
   ],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    text = re.sub(r\"http\\S+|www.\\S+\", \"\", text)   # remove URLs\n",
    "    text = re.sub(r\"@\\w+|#\\w+\", \"\", text)         # remove @user and hashtags\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)       # keep only letters\n",
    "    text = text.lower()\n",
    "    text = ' '.join([word for word in text.split() if word not in stop])\n",
    "    return text\n",
    "\n",
    "df['clean_text'] = df['tweet_text'].apply(clean_text)\n",
    "print(df[['tweet_text','clean_text']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa02b1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open(\"Q:\\\\glove.6B\\\\glove.6B.300d.txt\", encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "        embeddings_index[word] = coefs\n",
    "print(\"Loaded %s word vectors.\" % len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ce7213b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (47692, 300)\n"
     ]
    }
   ],
   "source": [
    "def get_glove_vector(text):\n",
    "    words = text.split()\n",
    "    if len(words) == 0:\n",
    "        return np.zeros(300)\n",
    "    vectors = [embeddings_index.get(w, np.zeros(300)) for w in words]\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "X_glove = np.vstack(df['clean_text'].apply(get_glove_vector))\n",
    "print(\"Feature matrix shape:\", X_glove.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8dc56ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced feature matrix: (47692, 100)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=100)   # paper reduced to 9000; smaller for demo\n",
    "X_pca = pca.fit_transform(X_glove)\n",
    "print(\"Reduced feature matrix:\", X_pca.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44b638bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df['cyberbullying_type'])   # column name may vary\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_pca, y, test_size=0.2, random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2685e5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created train_ds and test_ds with sizes: 38153 9539\n",
      "Example item from train_ds:\n",
      "{'input_ids': tensor([    0,   506, 24029,  1099,  1114,   224,  6587,  5738,  4181,   192,\n",
      "            2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor(4)}\n"
     ]
    }
   ],
   "source": [
    "# Prepare text-based train/test splits for the Hugging Face dataset (keeps indices aligned)\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Ensure 'clean_text' exists; if not, create it from 'tweet_text'\n",
    "if 'clean_text' not in df.columns:\n",
    "    if 'tweet_text' in df.columns:\n",
    "        import re\n",
    "        from nltk.corpus import stopwords\n",
    "        stop = set(stopwords.words('english'))\n",
    "        def _clean_text(t):\n",
    "            t = str(t)\n",
    "            t = re.sub(r\"http\\S+|www.\\S+\", \"\", t)\n",
    "            t = re.sub(r\"@\\w+|#\\w+\", \"\", t)\n",
    "            t = re.sub(r\"[^a-zA-Z\\s]\", \"\", t)\n",
    "            t = t.lower()\n",
    "            t = ' '.join([w for w in t.split() if w not in stop])\n",
    "            return t\n",
    "        df['clean_text'] = df['tweet_text'].apply(_clean_text)\n",
    "    else:\n",
    "        raise KeyError(\"Neither 'clean_text' nor 'tweet_text' found in df.\")\n",
    "\n",
    "# texts list aligned with labels in `y`\n",
    "texts = df['clean_text'].tolist()\n",
    "# If `y` was not created earlier, compute it here\n",
    "if 'y' not in globals():\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(df['cyberbullying_type'])\n",
    "\n",
    "# Split texts and labels together so indices remain aligned\n",
    "X_train_texts, X_test_texts, y_train, y_test = train_test_split(texts, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Create TweetDataset instances (requires TweetDataset class and tokenizer to be defined)\n",
    "# If the TweetDataset class hasn't been defined (cell order/execution), define a compatible fallback here.\n",
    "if 'TweetDataset' not in globals():\n",
    "    # import torch only if not already available to avoid duplicate imports\n",
    "    if 'torch' not in globals():\n",
    "        import torch\n",
    "    from torch.utils.data import Dataset\n",
    "\n",
    "    class TweetDataset(Dataset):\n",
    "        def __init__(self, texts, labels, tokenizer, max_len=64):\n",
    "            self.texts = texts\n",
    "            self.labels = labels\n",
    "            self.tokenizer = tokenizer\n",
    "            self.max_len = max_len\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.texts)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            text = str(self.texts[idx])\n",
    "            encoding = self.tokenizer.encode_plus(\n",
    "                text,\n",
    "                add_special_tokens=True,\n",
    "                max_length=self.max_len,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            return {\n",
    "                'input_ids': encoding['input_ids'].flatten(),\n",
    "                'attention_mask': encoding['attention_mask'].flatten(),\n",
    "                'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "            }\n",
    "\n",
    "if 'tokenizer' not in globals():\n",
    "    from transformers import RobertaTokenizer\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "train_ds = TweetDataset(X_train_texts, y_train, tokenizer)\n",
    "test_ds = TweetDataset(X_test_texts, y_test, tokenizer)\n",
    "print('Created train_ds and test_ds with sizes:', len(train_ds), len(test_ds))\n",
    "print('Example item from train_ds:')\n",
    "print(train_ds[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730cb111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping keras as it is not installed.\n",
      "WARNING: Skipping tensorflow as it is not installed.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall -y keras tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13f6acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement tensorflow==2.14 (from versions: 2.20.0rc0, 2.20.0)\n",
      "ERROR: No matching distribution found for tensorflow==2.14\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow==2.14 keras==2.14\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd363a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in q:\\anaconda3\\lib\\site-packages (2.9.0)\n",
      "Requirement already satisfied: torchvision in q:\\anaconda3\\lib\\site-packages (0.24.0)\n",
      "Requirement already satisfied: torchaudio in q:\\anaconda3\\lib\\site-packages (2.9.0)\n",
      "Requirement already satisfied: transformers in q:\\anaconda3\\lib\\site-packages (4.57.1)\n",
      "Requirement already satisfied: filelock in q:\\anaconda3\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in q:\\anaconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in q:\\anaconda3\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in q:\\anaconda3\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in q:\\anaconda3\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in q:\\anaconda3\\lib\\site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in q:\\anaconda3\\lib\\site-packages (from torch) (72.1.0)\n",
      "Requirement already satisfied: numpy in q:\\anaconda3\\lib\\site-packages (from torchvision) (2.1.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in q:\\anaconda3\\lib\\site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in q:\\anaconda3\\lib\\site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: packaging>=20.0 in q:\\anaconda3\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in q:\\anaconda3\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in q:\\anaconda3\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in q:\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in q:\\anaconda3\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in q:\\anaconda3\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in q:\\anaconda3\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in q:\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in q:\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in q:\\anaconda3\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in q:\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in q:\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in q:\\anaconda3\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in q:\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.10.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision torchaudio transformers --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f038aa18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Installed accelerate and transformers[torch] (if not already). You may need to restart the kernel after installation.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Installed accelerate and transformers[torch] (if not already). You may need to restart the kernel after installation.\n"
     ]
    }
   ],
   "source": [
    "# Install accelerate (required by Trainer) and torch extras for transformers; run in the notebook kernel.\n",
    "%pip install \"accelerate>=0.26.0\" --quiet\n",
    "%pip install \"transformers[torch]\" --quiet\n",
    "print('Installed accelerate and transformers[torch] (if not already). You may need to restart the kernel after installation.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "188f9513",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 63\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m# Fallback for older transformers: use eval_steps/save_steps instead (one per epoch)\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     steps_per_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(train_ds) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m common_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mper_device_train_batch_size\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 63\u001b[0m     training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m     64\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcommon_args,\n\u001b[0;32m     65\u001b[0m         eval_steps\u001b[38;5;241m=\u001b[39msteps_per_epoch,\n\u001b[0;32m     66\u001b[0m         save_steps\u001b[38;5;241m=\u001b[39msteps_per_epoch,\n\u001b[0;32m     67\u001b[0m     )\n\u001b[0;32m     68\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     69\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     70\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m     71\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_ds,\n\u001b[0;32m     72\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mtest_ds,\n\u001b[0;32m     73\u001b[0m )\n\u001b[0;32m     74\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain() \n",
      "File \u001b[1;32m<string>:135\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, parallelism_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, project, trackio_space_id, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, hub_revision, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, liger_kernel_config, eval_use_gather_object, average_tokens_across_devices)\u001b[0m\n",
      "File \u001b[1;32mq:\\Anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1811\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1809\u001b[0m \u001b[38;5;66;03m# Initialize device before we proceed\u001b[39;00m\n\u001b[0;32m   1810\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_torch_available():\n\u001b[1;32m-> 1811\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m   1813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtorchdynamo \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1814\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1815\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torchdynamo` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1816\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `torch_compile_backend` instead\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1817\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m   1818\u001b[0m     )\n",
      "File \u001b[1;32mq:\\Anaconda3\\Lib\\site-packages\\transformers\\training_args.py:2355\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2351\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2352\u001b[0m \u001b[38;5;124;03mThe device used by this process.\u001b[39;00m\n\u001b[0;32m   2353\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2354\u001b[0m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m-> 2355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_devices\n",
      "File \u001b[1;32mq:\\Anaconda3\\Lib\\functools.py:1026\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[1;34m(self, instance, owner)\u001b[0m\n\u001b[0;32m   1024\u001b[0m val \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname, _NOT_FOUND)\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[1;32m-> 1026\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(instance)\n\u001b[0;32m   1027\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1028\u001b[0m         cache[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname] \u001b[38;5;241m=\u001b[39m val\n",
      "File \u001b[1;32mq:\\Anaconda3\\Lib\\site-packages\\transformers\\training_args.py:2225\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[0;32m   2224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[1;32m-> 2225\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m   2226\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2227\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease run `pip install transformers[torch]` or `pip install \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2228\u001b[0m         )\n\u001b[0;32m   2229\u001b[0m \u001b[38;5;66;03m# We delay the init of `PartialState` to the end for clarity\u001b[39;00m\n\u001b[0;32m   2230\u001b[0m accelerator_state_kwargs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menabled\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_configured_state\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n",
      "\u001b[1;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import inspect\n",
    "\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=64):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "# Prefer text-based datasets created earlier (train_ds/test_ds). If they don't exist, create from text splits.\n",
    "if 'train_ds' not in globals():\n",
    "    if 'X_train_texts' in globals():\n",
    "        train_ds = TweetDataset(X_train_texts, y_train, tokenizer)\n",
    "        test_ds = TweetDataset(X_test_texts, y_test, tokenizer)\n",
    "    else:\n",
    "        raise NameError('train_ds not found and text-based splits are missing. Run preprocessing cells first.')\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=len(np.unique(y)))\n",
    "# Build TrainingArguments in a backward-compatible way\n",
    "common_args = dict(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "\n",
    ")\n",
    "\n",
    "# Check if TrainingArguments supports 'evaluation_strategy' (newer transformers)\n",
    "if 'evaluation_strategy' in inspect.signature(TrainingArguments.__init__).parameters:\n",
    "    training_args = TrainingArguments(\n",
    "        **common_args,\n",
    "        evaluation_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "    )\n",
    "else:\n",
    "    # Fallback for older transformers: use eval_steps/save_steps instead (one per epoch)\n",
    "    steps_per_epoch = max(1, len(train_ds) // common_args['per_device_train_batch_size'])\n",
    "    training_args = TrainingArguments(\n",
    "        **common_args,\n",
    "        eval_steps=steps_per_epoch,\n",
    "        save_steps=steps_per_epoch,\n",
    "    )\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    ")\n",
    "trainer.train() \n",
    "print(\"Training complete.\")# Cyberbullying Detection using GloVe and RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b0deb3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m metrics \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(metrics)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate()\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d63d936",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lime'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlime\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlime_text\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LimeTextExplainer\n\u001b[0;32m      3\u001b[0m explainer \u001b[38;5;241m=\u001b[39m LimeTextExplainer(class_names\u001b[38;5;241m=\u001b[39mle\u001b[38;5;241m.\u001b[39mclasses_)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict_proba\u001b[39m(texts):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'lime'"
     ]
    }
   ],
   "source": [
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "explainer = LimeTextExplainer(class_names=le.classes_)\n",
    "\n",
    "def predict_proba(texts):\n",
    "    tokens = tokenizer(texts, padding=True, truncation=True, max_length=64, return_tensors='pt')\n",
    "    outputs = model(**tokens)\n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=1).detach().numpy()\n",
    "    return probs\n",
    "\n",
    "idx = 0\n",
    "exp = explainer.explain_instance(df['clean_text'].iloc[idx], predict_proba, num_features=10)\n",
    "exp.show_in_notebook(text=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4a5939",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "models = {\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"SVM\": SVC(kernel='linear', probability=True, random_state=42),\n",
    "    \"NaiveBayes\": GaussianNB(),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5)\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, average='macro')\n",
    "    rec = recall_score(y_test, y_pred, average='macro')\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    results.append([name, acc, prec, rec, f1])\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1'])\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c60a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "cv_acc = cross_val_score(rf, X_pca, y, cv=kf, scoring='accuracy')\n",
    "cv_prec = cross_val_score(rf, X_pca, y, cv=kf, scoring='precision_macro')\n",
    "cv_rec = cross_val_score(rf, X_pca, y, cv=kf, scoring='recall_macro')\n",
    "cv_f1 = cross_val_score(rf, X_pca, y, cv=kf, scoring='f1_macro')\n",
    "\n",
    "print(f\"Average Accuracy: {cv_acc.mean():.2f}\")\n",
    "print(f\"Average Precision: {cv_prec.mean():.2f}\")\n",
    "print(f\"Average Recall: {cv_rec.mean():.2f}\")\n",
    "print(f\"Average F1 Score: {cv_f1.mean():.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
