{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50c4e16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet torch scikit-learn pandas numpy nltk lime tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "208d7feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\krish'\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from lime.lime_text import LimeTextExplainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "897b1b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# File + column names (you confirmed these)\n",
    "DATA_FILE = \"cyberbullying_tweets.csv\"\n",
    "TEXT_COL = \"tweet_text\"\n",
    "LABEL_COL = \"cyberbullying_type\"\n",
    "\n",
    "# GloVe file path (download and extract glove.6B.300d.txt)\n",
    "GLOVE_FILE = \"glove.6B.300d.txt\"\n",
    "\n",
    "# PCA target (paper uses 9000). If your machine can't handle it, change to lower (e.g., 300, 1000).\n",
    "PCA_N_COMPONENTS = 9000\n",
    "\n",
    "# Training hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 10\n",
    "LR = 1e-4\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19d17e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset: (47692, 2)\n",
      "After dropping NA: (47692, 2)\n"
     ]
    }
   ],
   "source": [
    "assert os.path.exists(DATA_FILE), f\"Dataset file not found: {DATA_FILE}\"\n",
    "df = pd.read_csv(DATA_FILE)\n",
    "print(\"Loaded dataset:\", df.shape)\n",
    "if TEXT_COL not in df.columns or LABEL_COL not in df.columns:\n",
    "    raise KeyError(f\"Expected columns '{TEXT_COL}' and '{LABEL_COL}' in {DATA_FILE}. Found: {list(df.columns)}\")\n",
    "df = df[[TEXT_COL, LABEL_COL]].dropna().reset_index(drop=True)\n",
    "print(\"After dropping NA:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "265cb19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample cleaned text:\n",
      "['words food crapilicious', 'white', 'classy whore red velvet cupcakes']\n"
     ]
    }
   ],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "def clean_text(t):\n",
    "    t = str(t)\n",
    "    t = re.sub(r\"http\\S+|www\\.\\S+\", \"\", t)\n",
    "    t = re.sub(r\"@\\w+|#\\w+\", \"\", t)\n",
    "    t = re.sub(r\"[^a-zA-Z\\s]\", \" \", t)\n",
    "    t = t.lower()\n",
    "    t = \" \".join([w for w in t.split() if w and w not in stop])\n",
    "    return t\n",
    "\n",
    "df['clean_text'] = df[TEXT_COL].apply(clean_text)\n",
    "print(\"Sample cleaned text:\")\n",
    "print(df['clean_text'].head(3).to_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc44d0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading GloVe: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading GloVe: 400000it [00:41, 9630.14it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded GloVe vectors: 400000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "assert os.path.exists(GLOVE_FILE), f\"GloVe file not found: {GLOVE_FILE}. Download and extract glove.6B.300d.txt.\"\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(GLOVE_FILE, 'r', encoding='utf8') as f:\n",
    "    for line in tqdm(f, desc=\"Loading GloVe\"):\n",
    "        parts = line.rstrip().split(\" \")\n",
    "        word = parts[0]\n",
    "        vec = np.asarray(parts[1:], dtype=np.float32)\n",
    "        embeddings_index[word] = vec\n",
    "print(\"Loaded GloVe vectors:\", len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6012bd13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GloVe transform:   0%|          | 0/47692 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GloVe transform: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47692/47692 [00:03<00:00, 14220.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_glove shape: (47692, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_glove_vector(text):\n",
    "    words = text.split()\n",
    "    if not words:\n",
    "        return np.zeros(300, dtype=np.float32)\n",
    "    vecs = [embeddings_index.get(w) for w in words]\n",
    "    # replace missing with zeros\n",
    "    vecs = [v if v is not None else np.zeros(300, dtype=np.float32) for v in vecs]\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "# generate features (this may take a moment)\n",
    "X_glove = np.vstack([get_glove_vector(t) for t in tqdm(df['clean_text'], desc=\"GloVe transform\")])\n",
    "print(\"X_glove shape:\", X_glove.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16db458b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting PCA with n_components=9000 ...\n",
      "PCA with requested components failed: n_components=9000 must be between 0 and min(n_samples, n_features)=300 with svd_solver='covariance_eigh'\n",
      "Falling back to PCA n_components=300\n",
      "PCA done (fallback). Shape: (47692, 300)\n",
      "Explained variance ratio sum: (47692, 300) 0.99999994\n",
      "PCA done (fallback). Shape: (47692, 300)\n",
      "Explained variance ratio sum: (47692, 300) 0.99999994\n"
     ]
    }
   ],
   "source": [
    "# Try to run PCA with the paper's component count; fallback if memory error\n",
    "try:\n",
    "    print(f\"Attempting PCA with n_components={PCA_N_COMPONENTS} ...\")\n",
    "    pca = PCA(n_components=PCA_N_COMPONENTS, random_state=42)\n",
    "    X_pca = pca.fit_transform(X_glove)\n",
    "    print(\"PCA done. Shape:\", X_pca.shape)\n",
    "except Exception as e:\n",
    "    print(\"PCA with requested components failed:\", str(e))\n",
    "    # fallback to smaller PCA (300)\n",
    "    fallback = 300\n",
    "    print(f\"Falling back to PCA n_components={fallback}\")\n",
    "    pca = PCA(n_components=fallback, random_state=42)\n",
    "    X_pca = pca.fit_transform(X_glove)\n",
    "    print(\"PCA done (fallback). Shape:\", X_pca.shape)\n",
    "\n",
    "# Keep for later\n",
    "print(\"Explained variance ratio sum:\", X_pca.shape, np.sum(pca.explained_variance_ratio_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51ba5e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['age' 'ethnicity' 'gender' 'not_cyberbullying' 'other_cyberbullying'\n",
      " 'religion'] n_classes: 6\n",
      "Train/Test shapes: (38153, 300) (9539, 300)\n"
     ]
    }
   ],
   "source": [
    "# encode labels\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df[LABEL_COL].values)\n",
    "classes = le.classes_\n",
    "print(\"Classes:\", classes, \"n_classes:\", len(classes))\n",
    "\n",
    "# train/test split (80/20)\n",
    "X_train_np, X_test_np, y_train_np, y_test_np = train_test_split(\n",
    "    X_pca, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(\"Train/Test shapes:\", X_train_np.shape, X_test_np.shape)\n",
    "# convert to torch tensors for DL models\n",
    "X_train = torch.tensor(X_train_np, dtype=torch.float32)\n",
    "X_test  = torch.tensor(X_test_np, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train_np, dtype=torch.long)\n",
    "y_test  = torch.tensor(y_test_np, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fdcf391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model with input_dim=300, num_classes=6\n",
      "RoBERTaNetLike(\n",
      "  (input_proj): Linear(in_features=300, out_features=304, bias=True)\n",
      "  (mha): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=304, out_features=304, bias=True)\n",
      "  )\n",
      "  (norm1): LayerNorm((304,), eps=1e-05, elementwise_affine=True)\n",
      "  (ff): Sequential(\n",
      "    (0): Linear(in_features=304, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=304, bias=True)\n",
      "  )\n",
      "  (norm2): LayerNorm((304,), eps=1e-05, elementwise_affine=True)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=304, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=6, bias=True)\n",
      "  )\n",
      ")\n",
      "Logits shape: torch.Size([2, 6])\n",
      "Logits shape: torch.Size([2, 6])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class RoBERTaNetLike(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, n_heads=8, ff_dim=512, dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Project input to a dimension divisible by n_heads\n",
    "        self.embed_dim = n_heads * ((input_dim + n_heads - 1) // n_heads)  # next multiple of n_heads\n",
    "        self.input_proj = nn.Linear(input_dim, self.embed_dim)\n",
    "        \n",
    "        self.mha = nn.MultiheadAttention(embed_dim=self.embed_dim, num_heads=n_heads, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(self.embed_dim)\n",
    "        \n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(self.embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(ff_dim, self.embed_dim)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(self.embed_dim)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.embed_dim, ff_dim//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(ff_dim//2, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, input_dim)\n",
    "        x = self.input_proj(x)  # project to embed_dim\n",
    "        x_seq = x.unsqueeze(1)  # (batch, seq_len=1, embed_dim)\n",
    "        attn_out, _ = self.mha(x_seq, x_seq, x_seq)\n",
    "        attn_out = attn_out.squeeze(1)\n",
    "        x = self.norm1(x + attn_out)\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + ff_out)\n",
    "        logits = self.classifier(x)\n",
    "        return logits\n",
    "\n",
    "# Initialize model with actual dimensions from our data\n",
    "num_classes = len(le.classes_)\n",
    "input_dim = X_train.shape[1]  # this will be our PCA dimension\n",
    "print(f\"Creating model with input_dim={input_dim}, num_classes={num_classes}\")\n",
    "\n",
    "model = RoBERTaNetLike(input_dim=input_dim, num_classes=num_classes).to(DEVICE)\n",
    "print(model)\n",
    "\n",
    "# Quick forward pass test with a small batch from our data\n",
    "X_batch = X_train[:2].to(DEVICE)  # just use 2 examples to verify\n",
    "logits = model(X_batch)\n",
    "print(\"Logits shape:\", logits.shape)  # should be (batch_size, num_classes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d91f0509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training setup\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "# num_epochs = 10\n",
    "# batch_size = 64\n",
    "\n",
    "# train_dataset = torch.utils.data.TensorDataset(\n",
    "#     torch.FloatTensor(X_train),\n",
    "#     torch.LongTensor(y_train)\n",
    "# )\n",
    "# train_loader = torch.utils.data.DataLoader(\n",
    "#     train_dataset, \n",
    "#     batch_size=batch_size,\n",
    "#     shuffle=True\n",
    "# )\n",
    "\n",
    "# val_dataset = torch.utils.data.TensorDataset(\n",
    "#     torch.FloatTensor(X_test),\n",
    "#     torch.LongTensor(y_test)\n",
    "# )\n",
    "# val_loader = torch.utils.data.DataLoader(\n",
    "#     val_dataset,\n",
    "#     batch_size=batch_size,\n",
    "#     shuffle=False\n",
    "# )\n",
    "\n",
    "# # Training loop\n",
    "# best_val_acc = 0\n",
    "# for epoch in range(num_epochs):\n",
    "#     # Training phase\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "    \n",
    "#     for batch_X, batch_y in train_loader:\n",
    "#         batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(batch_X)\n",
    "#         loss = criterion(outputs, batch_y)\n",
    "        \n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         total_loss += loss.item()\n",
    "#         _, predicted = outputs.max(1)\n",
    "#         total += batch_y.size(0)\n",
    "#         correct += predicted.eq(batch_y).sum().item()\n",
    "    \n",
    "#     train_acc = 100. * correct / total\n",
    "#     avg_loss = total_loss / len(train_loader)\n",
    "    \n",
    "#     # Validation phase\n",
    "#     model.eval()\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     val_loss = 0\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for batch_X, batch_y in val_loader:\n",
    "#             batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n",
    "#             outputs = model(batch_X)\n",
    "#             loss = criterion(outputs, batch_y)\n",
    "            \n",
    "#             val_loss += loss.item()\n",
    "#             _, predicted = outputs.max(1)\n",
    "#             total += batch_y.size(0)\n",
    "#             correct += predicted.eq(batch_y).sum().item()\n",
    "    \n",
    "#     val_acc = 100. * correct / total\n",
    "#     avg_val_loss = val_loss / len(val_loader)\n",
    "    \n",
    "#     print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "#     print(f'Training - Loss: {avg_loss:.4f}, Accuracy: {train_acc:.2f}%')\n",
    "#     print(f'Validation - Loss: {avg_val_loss:.4f}, Accuracy: {val_acc:.2f}%')\n",
    "#     print('-' * 60)\n",
    "    \n",
    "#     # Save best model\n",
    "#     if val_acc > best_val_acc:\n",
    "#         best_val_acc = val_acc\n",
    "#         torch.save(model.state_dict(), 'best_model.pth')\n",
    "#         print(f'Saved new best model with validation accuracy: {val_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c94baad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - train_loss: 0.6589 - val_acc: 0.7951 val_f1: 0.7940\n",
      "Epoch 2/10 - train_loss: 0.4775 - val_acc: 0.8093 val_f1: 0.8069\n",
      "Epoch 2/10 - train_loss: 0.4775 - val_acc: 0.8093 val_f1: 0.8069\n",
      "Epoch 3/10 - train_loss: 0.4468 - val_acc: 0.8104 val_f1: 0.8112\n",
      "Epoch 3/10 - train_loss: 0.4468 - val_acc: 0.8104 val_f1: 0.8112\n",
      "Epoch 4/10 - train_loss: 0.4236 - val_acc: 0.8149 val_f1: 0.8149\n",
      "Epoch 4/10 - train_loss: 0.4236 - val_acc: 0.8149 val_f1: 0.8149\n",
      "Epoch 5/10 - train_loss: 0.4054 - val_acc: 0.8128 val_f1: 0.8109\n",
      "Epoch 5/10 - train_loss: 0.4054 - val_acc: 0.8128 val_f1: 0.8109\n",
      "Epoch 6/10 - train_loss: 0.3898 - val_acc: 0.8135 val_f1: 0.8134\n",
      "Epoch 6/10 - train_loss: 0.3898 - val_acc: 0.8135 val_f1: 0.8134\n",
      "Epoch 7/10 - train_loss: 0.3779 - val_acc: 0.8153 val_f1: 0.8159\n",
      "Epoch 7/10 - train_loss: 0.3779 - val_acc: 0.8153 val_f1: 0.8159\n",
      "Epoch 8/10 - train_loss: 0.3614 - val_acc: 0.8159 val_f1: 0.8139\n",
      "Epoch 8/10 - train_loss: 0.3614 - val_acc: 0.8159 val_f1: 0.8139\n",
      "Epoch 9/10 - train_loss: 0.3495 - val_acc: 0.8159 val_f1: 0.8166\n",
      "Epoch 9/10 - train_loss: 0.3495 - val_acc: 0.8159 val_f1: 0.8166\n",
      "Epoch 10/10 - train_loss: 0.3355 - val_acc: 0.8097 val_f1: 0.8104\n",
      "Best val F1 saved: 0.8166049921370869\n",
      "Epoch 10/10 - train_loss: 0.3355 - val_acc: 0.8097 val_f1: 0.8104\n",
      "Best val F1 saved: 0.8166049921370869\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# ‚úÖ Fixed RoBERTaNet-like model (auto adapts to PCA dimension)\n",
    "# ----------------------------------------------------------\n",
    "class RoBERTaNetLike(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, n_heads=4, ff_dim=None, dropout=0.3):\n",
    "        super().__init__()\n",
    "        if ff_dim is None:\n",
    "            ff_dim = input_dim * 2  # auto scale based on PCA dim\n",
    "\n",
    "        # Multi-head attention block\n",
    "        self.mha = nn.MultiheadAttention(embed_dim=input_dim, num_heads=n_heads, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(input_dim)\n",
    "\n",
    "        # Feed-forward block\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(input_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(ff_dim, input_dim)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(input_dim)\n",
    "\n",
    "        # Classifier head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_dim, input_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(input_dim // 2, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, features)\n",
    "        x_seq = x.unsqueeze(1)  # add fake sequence dim (batch, seq=1, features)\n",
    "        attn_out, _ = self.mha(x_seq, x_seq, x_seq)\n",
    "        attn_out = attn_out.squeeze(1)\n",
    "        x = self.norm1(x + attn_out)\n",
    "\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm2(x + ff_out)\n",
    "\n",
    "        logits = self.classifier(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# ‚úÖ Initialize model using your PCA dimension\n",
    "# ----------------------------------------------------------\n",
    "num_classes = len(np.unique(y))\n",
    "input_dim = X_train.shape[1]\n",
    "model = RoBERTaNetLike(input_dim=input_dim, num_classes=num_classes).to(DEVICE)\n",
    "\n",
    "print(f\"‚úÖ Model initialized with input_dim={input_dim}, num_classes={num_classes}\")\n",
    "print(model)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# ‚úÖ Create DataLoaders\n",
    "# ----------------------------------------------------------\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset  = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# ‚úÖ Loss & Optimizer\n",
    "# ----------------------------------------------------------\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# ‚úÖ Training & Evaluation Functions\n",
    "# ----------------------------------------------------------\n",
    "def train_one_epoch(model, loader):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(xb)\n",
    "        loss = criterion(out, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            out = model(xb)\n",
    "            preds.extend(torch.argmax(out, dim=1).cpu().numpy())\n",
    "            trues.extend(yb.numpy())\n",
    "    acc = accuracy_score(trues, preds)\n",
    "    prec = precision_score(trues, preds, average='macro', zero_division=0)\n",
    "    rec = recall_score(trues, preds, average='macro', zero_division=0)\n",
    "    f1 = f1_score(trues, preds, average='macro', zero_division=0)\n",
    "    return {\"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1\": f1}\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# ‚úÖ Training Loop\n",
    "# ----------------------------------------------------------\n",
    "best_val_f1 = 0.0\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    train_loss = train_one_epoch(model, train_loader)\n",
    "    metrics = evaluate(model, test_loader)\n",
    "    print(f\"Epoch {epoch}/{NUM_EPOCHS} | \"\n",
    "          f\"Train Loss: {train_loss:.4f} | \"\n",
    "          f\"Val Acc: {metrics['accuracy']:.4f} | \"\n",
    "          f\"Val F1: {metrics['f1']:.4f}\")\n",
    "    if metrics[\"f1\"] > best_val_f1:\n",
    "        best_val_f1 = metrics[\"f1\"]\n",
    "        torch.save(model.state_dict(), \"best_robertanet.pth\")\n",
    "\n",
    "print(\"üèÅ Training complete.\")\n",
    "print(\"üî• Best validation F1-score:\", best_val_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8f7e7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Map labels to 2 classes\n",
    "# y_train = torch.tensor([0 if l in [0,1,2] else 1 for l in y_train], dtype=torch.long)\n",
    "# y_test  = torch.tensor([0 if l in [0,1,2] else 1 for l in y_test], dtype=torch.long)\n",
    "\n",
    "# # Verify mapping\n",
    "# print(\"Unique labels in y_train after mapping:\", torch.unique(y_train))\n",
    "# print(\"Unique labels in y_test after mapping:\", torch.unique(y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad7fcfe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoBERTaNet-like final metrics: {'accuracy': 0.8159136177796414, 'precision': 0.8217898129474578, 'recall': 0.8153172778680111, 'f1': 0.8166049921370869}\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"best_robertanet.pth\"))\n",
    "final_metrics = evaluate(model, test_loader)\n",
    "print(\"RoBERTaNet-like final metrics:\", final_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5dbde240",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "q:\\Anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] The system cannot find the file specified\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"q:\\Anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "        \"wmic CPU Get NumberOfCores /Format:csv\".split(),\n",
      "        capture_output=True,\n",
      "        text=True,\n",
      "    )\n",
      "  File \"q:\\Anaconda3\\Lib\\subprocess.py\", line 554, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ~~~~~^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"q:\\Anaconda3\\Lib\\subprocess.py\", line 1039, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "                        pass_fds, cwd, env,\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "                        gid, gids, uid, umask,\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^\n",
      "                        start_new_session, process_group)\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"q:\\Anaconda3\\Lib\\subprocess.py\", line 1554, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^\n",
      "                             # no special security\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<4 lines>...\n",
      "                             cwd,\n",
      "                             ^^^^\n",
      "                             startupinfo)\n",
      "                             ^^^^^^^^^^^^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline ML results:\n",
      "           Model  Accuracy  Precision    Recall        F1\n",
      "0  RandomForest  0.743684   0.742201  0.742557  0.741313\n",
      "1           SVM  0.774609   0.770843  0.773873  0.770188\n",
      "2    NaiveBayes  0.482126   0.506838  0.481336  0.470877\n",
      "3           KNN  0.692525   0.671667  0.690988  0.660945\n"
     ]
    }
   ],
   "source": [
    "# Convert PCA arrays to numpy\n",
    "X_train_np = X_train.cpu().numpy()\n",
    "X_test_np  = X_test.cpu().numpy()\n",
    "y_train_np = y_train.cpu().numpy()\n",
    "y_test_np  = y_test.cpu().numpy()\n",
    "\n",
    "ml_models = {\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"SVM\": SVC(kernel='linear', probability=True, random_state=42),\n",
    "    \"NaiveBayes\": GaussianNB(),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5)\n",
    "}\n",
    "\n",
    "ml_results = []\n",
    "for name, clf in ml_models.items():\n",
    "    clf.fit(X_train_np, y_train_np)\n",
    "    preds = clf.predict(X_test_np)\n",
    "    ml_results.append([\n",
    "        name,\n",
    "        accuracy_score(y_test_np, preds),\n",
    "        precision_score(y_test_np, preds, average='macro', zero_division=0),\n",
    "        recall_score(y_test_np, preds, average='macro', zero_division=0),\n",
    "        f1_score(y_test_np, preds, average='macro', zero_division=0)\n",
    "    ])\n",
    "\n",
    "ml_df = pd.DataFrame(ml_results, columns=['Model','Accuracy','Precision','Recall','F1'])\n",
    "print(\"Baseline ML results:\\n\", ml_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "336f683c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep baseline results:\n",
      "     Model  Accuracy  Precision    Recall        F1\n",
      "0     CNN  0.336618   0.320126  0.335863  0.289882\n",
      "1  BiLSTM  0.804487   0.802498  0.803738  0.800928\n"
     ]
    }
   ],
   "source": [
    "# We use the PCA vector as sequence length = 1; for deeper baselines we reshape accordingly.\n",
    "# For a lightweight CNN/BiLSTM demo we'll adapt shapes but keep them simple.\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=1)\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1) # (B,1,D)\n",
    "        x = F.relu(self.conv(x)) # (B,64,D)\n",
    "        x = self.pool(x).squeeze(-1) # (B,64)\n",
    "        return self.fc(x)\n",
    "\n",
    "class SimpleBiLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden=128, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden*2, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # (B,1,D)\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :]\n",
    "        return self.fc(out)\n",
    "\n",
    "# instantiate\n",
    "cnn = SimpleCNN(X_train.shape[1], num_classes).to(DEVICE)\n",
    "bilstm = SimpleBiLSTM(X_train.shape[1], hidden=128, num_classes=num_classes).to(DEVICE)\n",
    "\n",
    "def train_and_eval_torch_model(torch_model, epochs=5):\n",
    "    torch_model.to(DEVICE)\n",
    "    opt = torch.optim.Adam(torch_model.parameters(), lr=1e-3)\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    for ep in range(epochs):\n",
    "        torch_model.train()\n",
    "        total=0\n",
    "        for xb, yb in train_loader:\n",
    "            xb,yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            opt.zero_grad()\n",
    "            loss = crit(torch_model(xb), yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total += loss.item()*xb.size(0)\n",
    "        # no verbose per batch\n",
    "    # eval\n",
    "    torch_model.eval()\n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            out = torch_model(xb)\n",
    "            preds.extend(torch.argmax(out, dim=1).cpu().numpy())\n",
    "            trues.extend(yb.numpy())\n",
    "    return accuracy_score(trues, preds), precision_score(trues, preds, average='macro', zero_division=0), recall_score(trues, preds, average='macro', zero_division=0), f1_score(trues, preds, average='macro', zero_division=0)\n",
    "\n",
    "cnn_metrics = train_and_eval_torch_model(cnn, epochs=5)\n",
    "bilstm_metrics = train_and_eval_torch_model(bilstm, epochs=5)\n",
    "deep_df = pd.DataFrame([\n",
    "    [\"CNN\", *cnn_metrics],\n",
    "    [\"BiLSTM\", *bilstm_metrics]\n",
    "], columns=['Model','Accuracy','Precision','Recall','F1'])\n",
    "print(\"Deep baseline results:\\n\", deep_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97a6c89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold CV (RandomForest):\n",
      "Accuracy: 0.7480291248312789\n",
      "Precision: 0.7463899129081428\n",
      "Recall: 0.746974082414235\n",
      "F1: 0.7453651825347777\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "cv_acc = cross_val_score(rf, X_pca, y, cv=kf, scoring='accuracy', n_jobs=-1)\n",
    "cv_prec = cross_val_score(rf, X_pca, y, cv=kf, scoring='precision_macro', n_jobs=-1)\n",
    "cv_rec = cross_val_score(rf, X_pca, y, cv=kf, scoring='recall_macro', n_jobs=-1)\n",
    "cv_f1 = cross_val_score(rf, X_pca, y, cv=kf, scoring='f1_macro', n_jobs=-1)\n",
    "\n",
    "print(\"5-fold CV (RandomForest):\")\n",
    "print(\"Accuracy:\", np.mean(cv_acc))\n",
    "print(\"Precision:\", np.mean(cv_prec))\n",
    "print(\"Recall:\", np.mean(cv_rec))\n",
    "print(\"F1:\", np.mean(cv_f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa8f0fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final comparison table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RoBERTaNet (PCA+GloVe)</td>\n",
       "      <td>0.815914</td>\n",
       "      <td>0.821790</td>\n",
       "      <td>0.815317</td>\n",
       "      <td>0.816605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BiLSTM</td>\n",
       "      <td>0.804487</td>\n",
       "      <td>0.802498</td>\n",
       "      <td>0.803738</td>\n",
       "      <td>0.800928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.774609</td>\n",
       "      <td>0.770843</td>\n",
       "      <td>0.773873</td>\n",
       "      <td>0.770188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.743684</td>\n",
       "      <td>0.742201</td>\n",
       "      <td>0.742557</td>\n",
       "      <td>0.741313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.692525</td>\n",
       "      <td>0.671667</td>\n",
       "      <td>0.690988</td>\n",
       "      <td>0.660945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaiveBayes</td>\n",
       "      <td>0.482126</td>\n",
       "      <td>0.506838</td>\n",
       "      <td>0.481336</td>\n",
       "      <td>0.470877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CNN</td>\n",
       "      <td>0.336618</td>\n",
       "      <td>0.320126</td>\n",
       "      <td>0.335863</td>\n",
       "      <td>0.289882</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Model  Accuracy  Precision    Recall        F1\n",
       "0  RoBERTaNet (PCA+GloVe)  0.815914   0.821790  0.815317  0.816605\n",
       "1                  BiLSTM  0.804487   0.802498  0.803738  0.800928\n",
       "2                     SVM  0.774609   0.770843  0.773873  0.770188\n",
       "3            RandomForest  0.743684   0.742201  0.742557  0.741313\n",
       "4                     KNN  0.692525   0.671667  0.690988  0.660945\n",
       "5              NaiveBayes  0.482126   0.506838  0.481336  0.470877\n",
       "6                     CNN  0.336618   0.320126  0.335863  0.289882"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Collect RoBERTaNet results + baselines into one table\n",
    "proposed = [\"RoBERTaNet (PCA+GloVe)\", final_metrics['accuracy'], final_metrics['precision'], final_metrics['recall'], final_metrics['f1']]\n",
    "all_results = pd.concat([ml_df, deep_df], ignore_index=True)\n",
    "all_results.loc[len(all_results)] = proposed\n",
    "all_results = all_results[['Model','Accuracy','Precision','Recall','F1']]\n",
    "print(\"Final comparison table:\")\n",
    "display(all_results.sort_values(by='Accuracy', ascending=False).reset_index(drop=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d51a2cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: In other words #katandandre, your food was crapilicious! #mkr\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(np.str_('food'), -0.002721404276564852),\n",
       " (np.str_('words'), -0.001109889114536454),\n",
       " (np.str_('crapilicious'), 0.00026098344418575226)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Original: Why is #aussietv so white? #MKR #theblock #ImACelebrityAU #today #sunrise #studio10 #Neighbours #WonderlandTen #etc\n",
      "Original: Why is #aussietv so white? #MKR #theblock #ImACelebrityAU #today #sunrise #studio10 #Neighbours #WonderlandTen #etc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(np.str_('white'), -1.613531223947455e-05)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Original: @XochitlSuckkks a classy whore? Or more red velvet cupcakes?\n",
      "Original: @XochitlSuckkks a classy whore? Or more red velvet cupcakes?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(np.str_('whore'), 0.0053927520000426514),\n",
       " (np.str_('classy'), -0.004928811283545458),\n",
       " (np.str_('velvet'), 0.002638974555206073),\n",
       " (np.str_('red'), 0.0021224102262066575),\n",
       " (np.str_('cupcakes'), 0.0015056497165607014)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# LIME expects a function that returns class probabilities for a list of raw texts.\n",
    "explainer = LimeTextExplainer(class_names=list(classes))\n",
    "\n",
    "def predict_proba_from_texts(texts):\n",
    "    # texts -> glove -> pca -> model -> softmax probs\n",
    "    X_tmp = np.vstack([get_glove_vector(clean_text(t)) for t in texts])\n",
    "    X_tmp_pca = pca.transform(X_tmp)\n",
    "    X_tmp_tensor = torch.tensor(X_tmp_pca, dtype=torch.float32).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        logits = model(X_tmp_tensor)\n",
    "        probs = F.softmax(logits, dim=1).cpu().numpy()\n",
    "    return probs\n",
    "\n",
    "# pick some examples and explain\n",
    "for idx in range(3):\n",
    "    text = df['clean_text'].iloc[idx]\n",
    "    exp = explainer.explain_instance(text, predict_proba_from_texts, num_features=10)\n",
    "    print(\"Original:\", df[TEXT_COL].iloc[idx])\n",
    "    display(exp.as_list())  # prints (feature, weight) pairs\n",
    "    print(\"-\"*80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
