{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50c4e16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet torch scikit-learn pandas numpy nltk lime tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "208d7feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\krish'\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from lime.lime_text import LimeTextExplainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "897b1b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    " \n",
    "DATA_FILE = \"cyberbullying_tweets.csv\"\n",
    "TEXT_COL = \"tweet_text\"\n",
    "LABEL_COL = \"cyberbullying_type\"\n",
    "\n",
    "# GloVe file path (download and extract glove.6B.300d.txt)\n",
    "GLOVE_FILE = \"glove.6B.300d.txt\"\n",
    "\n",
    "# PCA target (paper uses 9000). If your machine can't handle it, change to lower (e.g., 300, 1000).\n",
    "PCA_N_COMPONENTS = 9000\n",
    "\n",
    "# Training hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 10\n",
    "LR = 1e-4\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19d17e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset: (47692, 2)\n",
      "After dropping NA: (47692, 2)\n",
      "Label classes: ['age', 'ethnicity', 'gender', 'not_cyberbullying', 'other_cyberbullying', 'religion']\n"
     ]
    }
   ],
   "source": [
    "assert os.path.exists(DATA_FILE), f\"Dataset file not found: {DATA_FILE}\"\n",
    "\n",
    "# Load dataset first, then validate and encode labels\n",
    "df = pd.read_csv(DATA_FILE)\n",
    "print(\"Loaded dataset:\", df.shape)\n",
    "if TEXT_COL not in df.columns or LABEL_COL not in df.columns:\n",
    "    raise KeyError(f\"Expected columns '{TEXT_COL}' and '{LABEL_COL}' in {DATA_FILE}. Found: {list(df.columns)}\")\n",
    "df = df[[TEXT_COL, LABEL_COL]].dropna().reset_index(drop=True)\n",
    "print(\"After dropping NA:\", df.shape)\n",
    "\n",
    "le = LabelEncoder()\n",
    "df[LABEL_COL] = le.fit_transform(df[LABEL_COL])\n",
    "print(\"Label classes:\", list(le.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "265cb19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample cleaned text:\n",
      "['words food crapilicious', 'white', 'classy whore red velvet cupcakes']\n",
      "After removing empty tweets: (47199, 3)\n"
     ]
    }
   ],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "def clean_text(t):\n",
    "    t = str(t)\n",
    "    t = re.sub(r\"http\\S+|www\\.\\S+\", \"\", t)\n",
    "    t = re.sub(r\"@\\w+|#\\w+\", \"\", t)\n",
    "    t = re.sub(r\"[^a-zA-Z\\s]\", \" \", t)\n",
    "    t = t.lower()\n",
    "    t = \" \".join([w for w in t.split() if w and w not in stop])\n",
    "    return t\n",
    "\n",
    "df['clean_text'] = df[TEXT_COL].apply(clean_text)\n",
    "print(\"Sample cleaned text:\")\n",
    "print(df['clean_text'].head(3).to_list())\n",
    "df = df[df['clean_text'].str.strip() != \"\"].reset_index(drop=True)\n",
    "print(\"After removing empty tweets:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc44d0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading GloVe: 400000it [00:19, 20336.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded GloVe vectors: 400000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "assert os.path.exists(GLOVE_FILE), f\"GloVe file not found: {GLOVE_FILE}. Download and extract glove.6B.300d.txt.\"\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(GLOVE_FILE, 'r', encoding='utf8') as f:\n",
    "    for line in tqdm(f, desc=\"Loading GloVe\"):\n",
    "        parts = line.rstrip().split(\" \")\n",
    "        word = parts[0]\n",
    "        vec = np.asarray(parts[1:], dtype=np.float32)\n",
    "        embeddings_index[word] = vec\n",
    "print(\"Loaded GloVe vectors:\", len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6012bd13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 9000D tweet embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 47199/47199 [00:06<00:00, 6878.22it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… X_glove shape: (47199, 9000)\n"
     ]
    }
   ],
   "source": [
    "MAX_TOKENS = 30\n",
    "EMBED_DIM = 300\n",
    "\n",
    "def get_glove_vector_sequence(text):\n",
    "    words = text.split()\n",
    "    vecs = []\n",
    "    for w in words[:MAX_TOKENS]:\n",
    "        vecs.append(embeddings_index.get(w, np.zeros(EMBED_DIM)))\n",
    "    # pad if tweet has fewer than 30 tokens\n",
    "    while len(vecs) < MAX_TOKENS:\n",
    "        vecs.append(np.zeros(EMBED_DIM))\n",
    "    return np.array(vecs).flatten()  # 30 Ã— 300 = 9000 features\n",
    "\n",
    "print(\"Generating 9000D tweet embeddings...\")\n",
    "X_glove = np.vstack([get_glove_vector_sequence(t) for t in tqdm(df['clean_text'])])\n",
    "print(\"âœ… X_glove shape:\", X_glove.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16db458b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running PCA with n_components=9000 on training data ...\n",
      "âœ… PCA done. Train shape: (37759, 9000) | Test shape: (9440, 9000)\n",
      "Explained variance ratio sum: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 1ï¸âƒ£ Split first (80:20, stratified)\n",
    "X_train_glove, X_test_glove, y_train, y_test = train_test_split(\n",
    "    X_glove, df[LABEL_COL].values,\n",
    "    test_size=0.2, random_state=42, stratify=df[LABEL_COL].values\n",
    ")\n",
    "\n",
    "# 2ï¸âƒ£ Apply PCA on training data only (auto-adjust if fewer features)\n",
    "max_possible = X_train_glove.shape[1]\n",
    "n_comp = min(PCA_N_COMPONENTS, max_possible)\n",
    "\n",
    "print(f\"Running PCA with n_components={n_comp} on training data ...\")\n",
    "try:\n",
    "    pca = PCA(n_components=n_comp, random_state=42)\n",
    "    X_train = pca.fit_transform(X_train_glove)\n",
    "    X_test  = pca.transform(X_test_glove)\n",
    "    print(\"âœ… PCA done. Train shape:\", X_train.shape, \"| Test shape:\", X_test.shape)\n",
    "    print(\"Explained variance ratio sum:\", np.sum(pca.explained_variance_ratio_))\n",
    "\n",
    "    if n_comp < PCA_N_COMPONENTS:\n",
    "        print(f\"âš ï¸ Note: Requested {PCA_N_COMPONENTS} components, but only {max_possible} available \"\n",
    "              f\"(input feature dimension). Using {n_comp}.\")\n",
    "except Exception as e:\n",
    "    print(\"âŒ PCA failed:\", str(e))\n",
    "    fallback = max_possible\n",
    "    print(f\"Falling back to PCA n_components={fallback}\")\n",
    "    pca = PCA(n_components=fallback, random_state=42)\n",
    "    X_train = pca.fit_transform(X_train_glove)\n",
    "    X_test  = pca.transform(X_test_glove)\n",
    "    print(\"âœ… PCA done (fallback). Train shape:\", X_train.shape, \"| Test shape:\", X_test.shape)\n",
    "    print(\"Explained variance ratio sum:\", np.sum(pca.explained_variance_ratio_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51ba5e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: [0 1 2 3 4 5] | n_classes: 6\n",
      "âœ… Train/Test tensor shapes:\n",
      "   X_train: torch.Size([37759, 9000])\n",
      "   X_test : torch.Size([9440, 9000])\n",
      "   y_train: torch.Size([37759]) dtype: torch.int64\n",
      "   y_test : torch.Size([9440]) dtype: torch.int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "\n",
    "# 1ï¸âƒ£ Encode labels before split\n",
    "le = LabelEncoder()\n",
    "y_all = le.fit_transform(df[LABEL_COL].values)\n",
    "classes = le.classes_\n",
    "print(\"Classes:\", classes, \"| n_classes:\", len(classes))\n",
    "\n",
    "# 2ï¸âƒ£ Reuse the same stratified split logic from PCA cell\n",
    "# (But with encoded numeric labels)\n",
    "from sklearn.model_selection import train_test_split\n",
    "y_train_np, y_test_np = train_test_split(\n",
    "    y_all, test_size=0.2, random_state=42, stratify=y_all\n",
    ")\n",
    "\n",
    "# 3ï¸âƒ£ Convert to torch tensors\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_t  = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train_np, dtype=torch.long)\n",
    "y_test_t  = torch.tensor(y_test_np, dtype=torch.long)\n",
    "\n",
    "print(\"âœ… Train/Test tensor shapes:\")\n",
    "print(\"   X_train:\", X_train_t.shape)\n",
    "print(\"   X_test :\", X_test_t.shape)\n",
    "print(\"   y_train:\", y_train_t.shape, \"dtype:\", y_train_t.dtype)\n",
    "print(\"   y_test :\", y_test_t.shape, \"dtype:\", y_test_t.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4fdcf391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model initialized with input_dim=9000 â†’ (30Ã—300), num_classes=6\n",
      "RoBERTaNetFull(\n",
      "  (mha1): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=300, out_features=300, bias=True)\n",
      "  )\n",
      "  (ff1): Sequential(\n",
      "    (0): Linear(in_features=300, out_features=600, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=600, out_features=300, bias=True)\n",
      "  )\n",
      "  (norm1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
      "  (norm2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
      "  (mha2): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=300, out_features=300, bias=True)\n",
      "  )\n",
      "  (ff2): Sequential(\n",
      "    (0): Linear(in_features=300, out_features=600, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=600, out_features=300, bias=True)\n",
      "  )\n",
      "  (norm3): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
      "  (norm4): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=300, out_features=150, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=150, out_features=6, bias=True)\n",
      "  )\n",
      ")\n",
      "Logits shape: torch.Size([2, 6])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Paper architecture constants\n",
    "D_MODEL = 300           # same as GloVe embedding size\n",
    "SEQ_LEN = 9000 // D_MODEL  # 30 tokens\n",
    "N_HEADS = 6             # âœ… divisible by 300\n",
    "DROPOUT = 0.3\n",
    "\n",
    "class RoBERTaNetFull(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Block 1\n",
    "        self.mha1 = nn.MultiheadAttention(embed_dim=D_MODEL, num_heads=N_HEADS, batch_first=True)\n",
    "        self.ff1  = nn.Sequential(\n",
    "            nn.Linear(D_MODEL, D_MODEL * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT),\n",
    "            nn.Linear(D_MODEL * 2, D_MODEL)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(D_MODEL)\n",
    "        self.norm2 = nn.LayerNorm(D_MODEL)\n",
    "\n",
    "        # Block 2\n",
    "        self.mha2 = nn.MultiheadAttention(embed_dim=D_MODEL, num_heads=N_HEADS, batch_first=True)\n",
    "        self.ff2  = nn.Sequential(\n",
    "            nn.Linear(D_MODEL, D_MODEL * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT),\n",
    "            nn.Linear(D_MODEL * 2, D_MODEL)\n",
    "        )\n",
    "        self.norm3 = nn.LayerNorm(D_MODEL)\n",
    "        self.norm4 = nn.LayerNorm(D_MODEL)\n",
    "\n",
    "        # Classifier head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(D_MODEL, D_MODEL // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT),\n",
    "            nn.Linear(D_MODEL // 2, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_flat):\n",
    "        # x_flat shape: (batch, 9000)\n",
    "        B = x_flat.size(0)\n",
    "        x = x_flat.view(B, SEQ_LEN, D_MODEL)   # (B, 30, 300)\n",
    "        \n",
    "        # Block 1\n",
    "        a1, _ = self.mha1(x, x, x)\n",
    "        x = self.norm1(x + a1)\n",
    "        x = self.norm2(x + self.ff1(x))\n",
    "\n",
    "        # Block 2\n",
    "        a2, _ = self.mha2(x, x, x)\n",
    "        x = self.norm3(x + a2)\n",
    "        x = self.norm4(x + self.ff2(x))\n",
    "\n",
    "        # Mean pooling (aggregate token features)\n",
    "        x = x.mean(dim=1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "# Initialize model\n",
    "num_classes = len(le.classes_)\n",
    "model = RoBERTaNetFull(num_classes=num_classes).to(DEVICE)\n",
    "\n",
    "print(f\"âœ… Model initialized with input_dim=9000 â†’ (30Ã—300), num_classes={num_classes}\")\n",
    "print(model)\n",
    "\n",
    "# Quick forward pass test\n",
    "X_batch = X_train_t[:2].to(DEVICE)\n",
    "logits = model(X_batch)\n",
    "print(\"Logits shape:\", logits.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d91f0509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training setup\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "# num_epochs = 10\n",
    "# batch_size = 64\n",
    "\n",
    "# train_dataset = torch.utils.data.TensorDataset(\n",
    "#     torch.FloatTensor(X_train),\n",
    "#     torch.LongTensor(y_train)\n",
    "# )\n",
    "# train_loader = torch.utils.data.DataLoader(\n",
    "#     train_dataset, \n",
    "#     batch_size=batch_size,\n",
    "#     shuffle=True\n",
    "# )\n",
    "\n",
    "# val_dataset = torch.utils.data.TensorDataset(\n",
    "#     torch.FloatTensor(X_test),\n",
    "#     torch.LongTensor(y_test)\n",
    "# )\n",
    "# val_loader = torch.utils.data.DataLoader(\n",
    "#     val_dataset,\n",
    "#     batch_size=batch_size,\n",
    "#     shuffle=False\n",
    "# )\n",
    "\n",
    "# # Training loop\n",
    "# best_val_acc = 0\n",
    "# for epoch in range(num_epochs):\n",
    "#     # Training phase\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "    \n",
    "#     for batch_X, batch_y in train_loader:\n",
    "#         batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(batch_X)\n",
    "#         loss = criterion(outputs, batch_y)\n",
    "        \n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         total_loss += loss.item()\n",
    "#         _, predicted = outputs.max(1)\n",
    "#         total += batch_y.size(0)\n",
    "#         correct += predicted.eq(batch_y).sum().item()\n",
    "    \n",
    "#     train_acc = 100. * correct / total\n",
    "#     avg_loss = total_loss / len(train_loader)\n",
    "    \n",
    "#     # Validation phase\n",
    "#     model.eval()\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     val_loss = 0\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for batch_X, batch_y in val_loader:\n",
    "#             batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)\n",
    "#             outputs = model(batch_X)\n",
    "#             loss = criterion(outputs, batch_y)\n",
    "            \n",
    "#             val_loss += loss.item()\n",
    "#             _, predicted = outputs.max(1)\n",
    "#             total += batch_y.size(0)\n",
    "#             correct += predicted.eq(batch_y).sum().item()\n",
    "    \n",
    "#     val_acc = 100. * correct / total\n",
    "#     avg_val_loss = val_loss / len(val_loader)\n",
    "    \n",
    "#     print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "#     print(f'Training - Loss: {avg_loss:.4f}, Accuracy: {train_acc:.2f}%')\n",
    "#     print(f'Validation - Loss: {avg_val_loss:.4f}, Accuracy: {val_acc:.2f}%')\n",
    "#     print('-' * 60)\n",
    "    \n",
    "#     # Save best model\n",
    "#     if val_acc > best_val_acc:\n",
    "#         best_val_acc = val_acc\n",
    "#         torch.save(model.state_dict(), 'best_model.pth')\n",
    "#         print(f'Saved new best model with validation accuracy: {val_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c94baad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model initialized with input_dim=9000 â†’ (30Ã—300), num_classes=6\n",
      "RoBERTaNetFull(\n",
      "  (mha1): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=300, out_features=300, bias=True)\n",
      "  )\n",
      "  (ff1): Sequential(\n",
      "    (0): Linear(in_features=300, out_features=600, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=600, out_features=300, bias=True)\n",
      "  )\n",
      "  (norm1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
      "  (norm2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
      "  (mha2): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=300, out_features=300, bias=True)\n",
      "  )\n",
      "  (ff2): Sequential(\n",
      "    (0): Linear(in_features=300, out_features=600, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=600, out_features=300, bias=True)\n",
      "  )\n",
      "  (norm3): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
      "  (norm4): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=300, out_features=150, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=150, out_features=6, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 1/15 | Train Loss: 1.0140 | Val Acc: 0.6537 | Val F1: 0.6422\n",
      "Epoch 2/15 | Train Loss: 0.8051 | Val Acc: 0.6839 | Val F1: 0.6732\n",
      "Epoch 3/15 | Train Loss: 0.7119 | Val Acc: 0.7166 | Val F1: 0.7152\n",
      "Epoch 4/15 | Train Loss: 0.6365 | Val Acc: 0.7266 | Val F1: 0.7261\n",
      "Epoch 5/15 | Train Loss: 0.5797 | Val Acc: 0.7428 | Val F1: 0.7386\n",
      "Epoch 6/15 | Train Loss: 0.5271 | Val Acc: 0.7435 | Val F1: 0.7388\n",
      "Epoch 7/15 | Train Loss: 0.4839 | Val Acc: 0.7430 | Val F1: 0.7348\n",
      "Epoch 8/15 | Train Loss: 0.4442 | Val Acc: 0.7476 | Val F1: 0.7474\n",
      "Epoch 9/15 | Train Loss: 0.4113 | Val Acc: 0.7457 | Val F1: 0.7429\n",
      "Epoch 10/15 | Train Loss: 0.3818 | Val Acc: 0.7434 | Val F1: 0.7378\n",
      "Epoch 11/15 | Train Loss: 0.3554 | Val Acc: 0.7392 | Val F1: 0.7404\n",
      "Epoch 12/15 | Train Loss: 0.3273 | Val Acc: 0.7374 | Val F1: 0.7335\n",
      "Epoch 13/15 | Train Loss: 0.3057 | Val Acc: 0.7355 | Val F1: 0.7362\n",
      "Epoch 14/15 | Train Loss: 0.2840 | Val Acc: 0.7381 | Val F1: 0.7354\n",
      "Epoch 15/15 | Train Loss: 0.2619 | Val Acc: 0.7288 | Val F1: 0.7301\n",
      "ðŸ Training complete.\n",
      "ðŸ”¥ Best validation F1-score: 0.7473797532100214\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# âœ… Ensure tensors are consistent\n",
    "# ----------------------------------------------------------\n",
    "# Use the _t versions (already encoded + converted)\n",
    "X_train_final = X_train_t.clone().detach()\n",
    "X_test_final  = X_test_t.clone().detach()\n",
    "y_train_final = y_train_t.clone().detach()\n",
    "y_test_final  = y_test_t.clone().detach()\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# âœ… Define RoBERTaNetFull Model (as per IEEE paper)\n",
    "# ----------------------------------------------------------\n",
    "class RoBERTaNetFull(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.D_MODEL = 300           # GloVe embedding size\n",
    "        self.SEQ_LEN = 9000 // self.D_MODEL  # 30 tokens\n",
    "        self.N_HEADS = 6             # âœ… must divide 300\n",
    "        self.DROPOUT = 0.3\n",
    "\n",
    "        # Transformer Block 1\n",
    "        self.mha1 = nn.MultiheadAttention(embed_dim=self.D_MODEL, num_heads=self.N_HEADS, batch_first=True)\n",
    "        self.ff1 = nn.Sequential(\n",
    "            nn.Linear(self.D_MODEL, self.D_MODEL * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.DROPOUT),\n",
    "            nn.Linear(self.D_MODEL * 2, self.D_MODEL)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(self.D_MODEL)\n",
    "        self.norm2 = nn.LayerNorm(self.D_MODEL)\n",
    "\n",
    "        # Transformer Block 2\n",
    "        self.mha2 = nn.MultiheadAttention(embed_dim=self.D_MODEL, num_heads=self.N_HEADS, batch_first=True)\n",
    "        self.ff2 = nn.Sequential(\n",
    "            nn.Linear(self.D_MODEL, self.D_MODEL * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.DROPOUT),\n",
    "            nn.Linear(self.D_MODEL * 2, self.D_MODEL)\n",
    "        )\n",
    "        self.norm3 = nn.LayerNorm(self.D_MODEL)\n",
    "        self.norm4 = nn.LayerNorm(self.D_MODEL)\n",
    "\n",
    "        # Classification Head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.D_MODEL, self.D_MODEL // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.DROPOUT),\n",
    "            nn.Linear(self.D_MODEL // 2, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_flat):\n",
    "        B = x_flat.size(0)\n",
    "        x = x_flat.view(B, self.SEQ_LEN, self.D_MODEL)  # reshape (9000 â†’ 30Ã—300)\n",
    "\n",
    "        # Transformer Block 1\n",
    "        a1, _ = self.mha1(x, x, x)\n",
    "        x = self.norm1(x + a1)\n",
    "        x = self.norm2(x + self.ff1(x))\n",
    "\n",
    "        # Transformer Block 2\n",
    "        a2, _ = self.mha2(x, x, x)\n",
    "        x = self.norm3(x + a2)\n",
    "        x = self.norm4(x + self.ff2(x))\n",
    "\n",
    "        # Mean pooling across sequence\n",
    "        x = x.mean(dim=1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# âœ… Initialize Model\n",
    "# ----------------------------------------------------------\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_classes = len(np.unique(y_train_final.numpy()))\n",
    "model = RoBERTaNetFull(num_classes=num_classes).to(DEVICE)\n",
    "\n",
    "print(f\"âœ… Model initialized with input_dim=9000 â†’ (30Ã—300), num_classes={num_classes}\")\n",
    "print(model)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# âœ… Create DataLoaders (use _final tensors)\n",
    "# ----------------------------------------------------------\n",
    "BATCH_SIZE = 64\n",
    "train_dataset = TensorDataset(X_train_final, y_train_final)\n",
    "test_dataset  = TensorDataset(X_test_final, y_test_final)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# âœ… Loss & Optimizer\n",
    "# ----------------------------------------------------------\n",
    "LR = 1e-4\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# âœ… Training & Evaluation Functions\n",
    "# ----------------------------------------------------------\n",
    "def train_one_epoch(model, loader):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(xb)\n",
    "        loss = criterion(out, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            out = model(xb)\n",
    "            preds.extend(torch.argmax(out, dim=1).cpu().numpy())\n",
    "            trues.extend(yb.numpy())\n",
    "    acc = accuracy_score(trues, preds)\n",
    "    prec = precision_score(trues, preds, average='macro', zero_division=0)\n",
    "    rec = recall_score(trues, preds, average='macro', zero_division=0)\n",
    "    f1 = f1_score(trues, preds, average='macro', zero_division=0)\n",
    "    return {\"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1\": f1}\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# âœ… Training Loop\n",
    "# ----------------------------------------------------------\n",
    "NUM_EPOCHS = 15\n",
    "best_val_f1 = 0.0\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    train_loss = train_one_epoch(model, train_loader)\n",
    "    metrics = evaluate(model, test_loader)\n",
    "    print(f\"Epoch {epoch}/{NUM_EPOCHS} | \"\n",
    "          f\"Train Loss: {train_loss:.4f} | \"\n",
    "          f\"Val Acc: {metrics['accuracy']:.4f} | \"\n",
    "          f\"Val F1: {metrics['f1']:.4f}\")\n",
    "    if metrics[\"f1\"] > best_val_f1:\n",
    "        best_val_f1 = metrics[\"f1\"]\n",
    "        torch.save(model.state_dict(), \"best_robertanet.pth\")\n",
    "\n",
    "print(\"ðŸ Training complete.\")\n",
    "print(\"ðŸ”¥ Best validation F1-score:\", best_val_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8f7e7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Map labels to 2 classes\n",
    "# y_train = torch.tensor([0 if l in [0,1,2] else 1 for l in y_train], dtype=torch.long)\n",
    "# y_test  = torch.tensor([0 if l in [0,1,2] else 1 for l in y_test], dtype=torch.long)\n",
    "\n",
    "# # Verify mapping\n",
    "# print(\"Unique labels in y_train after mapping:\", torch.unique(y_train))\n",
    "# print(\"Unique labels in y_test after mapping:\", torch.unique(y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad7fcfe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Final Evaluation on Test Set (IEEE Format):\n",
      "------------------------------------------------\n",
      "Accuracy : 0.7476\n",
      "Precision: 0.7576\n",
      "Recall   : 0.7453\n",
      "F1-score : 0.7474\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"best_robertanet.pth\"))\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "final_metrics = evaluate(model, test_loader)\n",
    "\n",
    "print(\"\\nðŸ“Š Final Evaluation on Test Set (IEEE Format):\")\n",
    "print(\"------------------------------------------------\")\n",
    "print(f\"Accuracy : {final_metrics['accuracy']:.4f}\")\n",
    "print(f\"Precision: {final_metrics['precision']:.4f}\")\n",
    "print(f\"Recall   : {final_metrics['recall']:.4f}\")\n",
    "print(f\"F1-score : {final_metrics['f1']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5dbde240",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 2.53 GiB for an array with shape (37759, 9000) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, clf \u001b[38;5;129;01min\u001b[39;00m ml_models\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSVM\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKNN\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m---> 31\u001b[0m         clf\u001b[38;5;241m.\u001b[39mfit(X_train_scaled, y_train_np)\n\u001b[0;32m     32\u001b[0m         preds \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(X_test_scaled)\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mq:\\Anaconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mq:\\Anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:246\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    245\u001b[0m         \u001b[38;5;66;03m# var = E[X^2] - E[X]^2 if sparse\u001b[39;00m\n\u001b[1;32m--> 246\u001b[0m         X_var \u001b[38;5;241m=\u001b[39m (X\u001b[38;5;241m.\u001b[39mmultiply(X))\u001b[38;5;241m.\u001b[39mmean() \u001b[38;5;241m-\u001b[39m (X\u001b[38;5;241m.\u001b[39mmean()) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sparse \u001b[38;5;28;01melse\u001b[39;00m X\u001b[38;5;241m.\u001b[39mvar()\n\u001b[0;32m    247\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gamma \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m (X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m X_var) \u001b[38;5;28;01mif\u001b[39;00m X_var \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1.0\u001b[39m\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mq:\\Anaconda3\\Lib\\site-packages\\numpy\\_core\\_methods.py:194\u001b[0m, in \u001b[0;36m_var\u001b[1;34m(a, axis, dtype, out, ddof, keepdims, where, mean)\u001b[0m\n\u001b[0;32m    189\u001b[0m         arrmean \u001b[38;5;241m=\u001b[39m arrmean \u001b[38;5;241m/\u001b[39m rcount\n\u001b[0;32m    191\u001b[0m \u001b[38;5;66;03m# Compute sum of squared deviations from mean\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;66;03m# Note that x may not be inexact and that we need it to be an array,\u001b[39;00m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;66;03m# not a scalar.\u001b[39;00m\n\u001b[1;32m--> 194\u001b[0m x \u001b[38;5;241m=\u001b[39m asanyarray(arr \u001b[38;5;241m-\u001b[39m arrmean)\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(arr\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, (nt\u001b[38;5;241m.\u001b[39mfloating, nt\u001b[38;5;241m.\u001b[39minteger)):\n\u001b[0;32m    197\u001b[0m     x \u001b[38;5;241m=\u001b[39m um\u001b[38;5;241m.\u001b[39mmultiply(x, x, out\u001b[38;5;241m=\u001b[39mx)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 2.53 GiB for an array with shape (37759, 9000) and data type float64"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "# Convert to numpy\n",
    "X_train_np = X_train_t.cpu().numpy()\n",
    "X_test_np  = X_test_t.cpu().numpy()\n",
    "y_train_np = y_train_t.cpu().numpy()\n",
    "y_test_np  = y_test_t.cpu().numpy()\n",
    "\n",
    "# Standardize features (for SVM and KNN)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_np)\n",
    "X_test_scaled  = scaler.transform(X_test_np)\n",
    "\n",
    "# Define models\n",
    "ml_models = {\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"SVM\": SVC(kernel='linear', probability=True, random_state=42),\n",
    "    \"NaiveBayes\": GaussianNB(),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5)\n",
    "}\n",
    "\n",
    "ml_results = []\n",
    "for name, clf in ml_models.items():\n",
    "    if name in [\"SVM\", \"KNN\"]:\n",
    "        clf.fit(X_train_scaled, y_train_np)\n",
    "        preds = clf.predict(X_test_scaled)\n",
    "    else:\n",
    "        clf.fit(X_train_np, y_train_np)\n",
    "        preds = clf.predict(X_test_np)\n",
    "    \n",
    "    ml_results.append([\n",
    "        name,\n",
    "        accuracy_score(y_test_np, preds),\n",
    "        precision_score(y_test_np, preds, average='macro', zero_division=0),\n",
    "        recall_score(y_test_np, preds, average='macro', zero_division=0),\n",
    "        f1_score(y_test_np, preds, average='macro', zero_division=0)\n",
    "    ])\n",
    "\n",
    "ml_df = pd.DataFrame(ml_results, columns=['Model','Accuracy','Precision','Recall','F1'])\n",
    "print(\"\\nðŸ“Š Baseline Machine Learning Results (After PCA):\\n\")\n",
    "print(ml_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "336f683c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Deep Baseline Results (After PCA 9000 â†’ 30Ã—300):\n",
      "\n",
      "    Model  Accuracy  Precision    Recall        F1\n",
      "0     CNN  0.529767   0.538390  0.528831  0.527219\n",
      "1  BiLSTM  0.265996   0.302001  0.268880  0.137180\n"
     ]
    }
   ],
   "source": [
    "# âœ… Corrected Deep Baseline Models (IEEE version)\n",
    "SEQ_LEN = 30\n",
    "FEAT_DIM = 300\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=FEAT_DIM, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.size(0)\n",
    "        x = x.view(B, FEAT_DIM, SEQ_LEN)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x).squeeze(-1)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class SimpleBiLSTM(nn.Module):\n",
    "    def __init__(self, hidden_dim=128, num_classes=6):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(FEAT_DIM, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.size(0)\n",
    "        x = x.view(B, SEQ_LEN, FEAT_DIM)\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :]\n",
    "        return self.fc(out)\n",
    "\n",
    "\n",
    "cnn = SimpleCNN(num_classes).to(DEVICE)\n",
    "bilstm = SimpleBiLSTM(hidden_dim=128, num_classes=num_classes).to(DEVICE)\n",
    "\n",
    "def train_and_eval_torch_model(torch_model, epochs=5):\n",
    "    torch_model.to(DEVICE)\n",
    "    opt = torch.optim.Adam(torch_model.parameters(), lr=1e-3)\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    for ep in range(epochs):\n",
    "        torch_model.train()\n",
    "        total = 0\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            opt.zero_grad()\n",
    "            loss = crit(torch_model(xb), yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total += loss.item() * xb.size(0)\n",
    "    torch_model.eval()\n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            out = torch_model(xb)\n",
    "            preds.extend(torch.argmax(out, dim=1).cpu().numpy())\n",
    "            trues.extend(yb.numpy())\n",
    "    return accuracy_score(trues, preds), precision_score(trues, preds, average='macro', zero_division=0), recall_score(trues, preds, average='macro', zero_division=0), f1_score(trues, preds, average='macro', zero_division=0)\n",
    "\n",
    "cnn_metrics = train_and_eval_torch_model(cnn, epochs=5)\n",
    "bilstm_metrics = train_and_eval_torch_model(bilstm, epochs=5)\n",
    "\n",
    "deep_df = pd.DataFrame([\n",
    "    [\"CNN\", *cnn_metrics],\n",
    "    [\"BiLSTM\", *bilstm_metrics]\n",
    "], columns=['Model','Accuracy','Precision','Recall','F1'])\n",
    "print(\"\\nðŸ“Š Deep Baseline Results (After PCA 9000 â†’ 30Ã—300):\\n\")\n",
    "print(deep_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97a6c89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running PCA (n_components=9000) for full-dataset CV...\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 3.16 GiB for an array with shape (47199, 9000) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning PCA (n_components=9000) for full-dataset CV...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m pca_full \u001b[38;5;241m=\u001b[39m PCA(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m9000\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m X_pca_full \u001b[38;5;241m=\u001b[39m pca_full\u001b[38;5;241m.\u001b[39mfit_transform(X_glove)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Define 5-fold stratified CV\u001b[39;00m\n\u001b[0;32m     11\u001b[0m skf \u001b[38;5;241m=\u001b[39m StratifiedKFold(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[1;32mq:\\Anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    325\u001b[0m         )\n",
      "File \u001b[1;32mq:\\Anaconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mq:\\Anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:468\u001b[0m, in \u001b[0;36mPCA.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    447\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model with X and apply the dimensionality reduction on X.\u001b[39;00m\n\u001b[0;32m    448\u001b[0m \n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;124;03m    C-ordered array, use 'np.ascontiguousarray'.\u001b[39;00m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 468\u001b[0m     U, S, _, X, x_is_centered, xp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X)\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m U \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    470\u001b[0m         U \u001b[38;5;241m=\u001b[39m U[:, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components_]\n",
      "File \u001b[1;32mq:\\Anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:542\u001b[0m, in \u001b[0;36mPCA._fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[38;5;66;03m# Call different fits for either full or truncated SVD\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcovariance_eigh\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_full(X, n_components, xp, is_array_api_compliant)\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marpack\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandomized\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_truncated(X, n_components, xp)\n",
      "File \u001b[1;32mq:\\Anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:571\u001b[0m, in \u001b[0;36mPCA._fit_full\u001b[1;34m(self, X, n_components, xp, is_array_api_compliant)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_ \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mreshape(xp\u001b[38;5;241m.\u001b[39masarray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_), (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,))\n\u001b[0;32m    570\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 571\u001b[0m     X_centered \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39masarray(X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;28;01melse\u001b[39;00m X\n\u001b[0;32m    572\u001b[0m     X_centered \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_\n\u001b[0;32m    573\u001b[0m     x_is_centered \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n",
      "File \u001b[1;32mq:\\Anaconda3\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:405\u001b[0m, in \u001b[0;36m_NumPyAPIWrapper.asarray\u001b[1;34m(self, x, dtype, device, copy)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;66;03m# Support copy in NumPy namespace\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 405\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m numpy\u001b[38;5;241m.\u001b[39marray(x, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m numpy\u001b[38;5;241m.\u001b[39masarray(x, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 3.16 GiB for an array with shape (47199, 9000) and data type float64"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Rebuild full PCA features (fit on all data just for CV)\n",
    "print(\"Running PCA (n_components=9000) for full-dataset CV...\")\n",
    "pca_full = PCA(n_components=9000, random_state=42)\n",
    "X_pca_full = pca_full.fit_transform(X_glove)\n",
    "\n",
    "# Define 5-fold stratified CV\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Perform 5-fold CV for each metric\n",
    "cv_acc = cross_val_score(rf, X_pca_full, y, cv=skf, scoring='accuracy', n_jobs=-1)\n",
    "cv_prec = cross_val_score(rf, X_pca_full, y, cv=skf, scoring='precision_macro', n_jobs=-1)\n",
    "cv_rec = cross_val_score(rf, X_pca_full, y, cv=skf, scoring='recall_macro', n_jobs=-1)\n",
    "cv_f1 = cross_val_score(rf, X_pca_full, y, cv=skf, scoring='f1_macro', n_jobs=-1)\n",
    "\n",
    "print(\"\\nðŸ“Š 5-Fold Cross-Validation (Random Forest on PCA-9000):\")\n",
    "print(\"------------------------------------------------------\")\n",
    "print(f\"Accuracy : {np.mean(cv_acc):.4f} Â± {np.std(cv_acc):.4f}\")\n",
    "print(f\"Precision: {np.mean(cv_prec):.4f}\")\n",
    "print(f\"Recall   : {np.mean(cv_rec):.4f}\")\n",
    "print(f\"F1-score : {np.mean(cv_f1):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8f0fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final comparison table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RoBERTaNet (PCA+GloVe)</td>\n",
       "      <td>0.815914</td>\n",
       "      <td>0.821790</td>\n",
       "      <td>0.815317</td>\n",
       "      <td>0.816605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BiLSTM</td>\n",
       "      <td>0.804487</td>\n",
       "      <td>0.802498</td>\n",
       "      <td>0.803738</td>\n",
       "      <td>0.800928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.774609</td>\n",
       "      <td>0.770843</td>\n",
       "      <td>0.773873</td>\n",
       "      <td>0.770188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.743684</td>\n",
       "      <td>0.742201</td>\n",
       "      <td>0.742557</td>\n",
       "      <td>0.741313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.692525</td>\n",
       "      <td>0.671667</td>\n",
       "      <td>0.690988</td>\n",
       "      <td>0.660945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaiveBayes</td>\n",
       "      <td>0.482126</td>\n",
       "      <td>0.506838</td>\n",
       "      <td>0.481336</td>\n",
       "      <td>0.470877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CNN</td>\n",
       "      <td>0.336618</td>\n",
       "      <td>0.320126</td>\n",
       "      <td>0.335863</td>\n",
       "      <td>0.289882</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Model  Accuracy  Precision    Recall        F1\n",
       "0  RoBERTaNet (PCA+GloVe)  0.815914   0.821790  0.815317  0.816605\n",
       "1                  BiLSTM  0.804487   0.802498  0.803738  0.800928\n",
       "2                     SVM  0.774609   0.770843  0.773873  0.770188\n",
       "3            RandomForest  0.743684   0.742201  0.742557  0.741313\n",
       "4                     KNN  0.692525   0.671667  0.690988  0.660945\n",
       "5              NaiveBayes  0.482126   0.506838  0.481336  0.470877\n",
       "6                     CNN  0.336618   0.320126  0.335863  0.289882"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ðŸ§© Combine Baselines + Deep Models + Proposed Model\n",
    "\n",
    "# Proposed model result (from RoBERTaNet evaluation)\n",
    "proposed = [\"RoBERTaNet (PCA+GloVe)\",\n",
    "            final_metrics['accuracy'],\n",
    "            final_metrics['precision'],\n",
    "            final_metrics['recall'],\n",
    "            final_metrics['f1']]\n",
    "\n",
    "# Combine everything\n",
    "all_results = pd.concat([ml_df, deep_df], ignore_index=True)\n",
    "all_results.loc[len(all_results)] = proposed\n",
    "all_results = all_results[['Model', 'Accuracy', 'Precision', 'Recall', 'F1']]\n",
    "\n",
    "# Round values for readability\n",
    "all_results[['Accuracy', 'Precision', 'Recall', 'F1']] = all_results[['Accuracy', 'Precision', 'Recall', 'F1']].astype(float).round(4)\n",
    "\n",
    "# Sort by accuracy (descending)\n",
    "all_results = all_results.sort_values(by='Accuracy', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"\\nðŸ“Š Final Model Comparison Table (Aligned with IEEE Table 7):\")\n",
    "print(\"-------------------------------------------------------------\")\n",
    "display(all_results.style.set_table_styles(\n",
    "    [{'selector': 'th', 'props': [('text-align', 'center')]},\n",
    "     {'selector': 'td', 'props': [('text-align', 'center')]}]\n",
    ").set_properties(**{'width': '100px'}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51a2cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: In other words #katandandre, your food was crapilicious! #mkr\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(np.str_('food'), -0.002721404276564852),\n",
       " (np.str_('words'), -0.001109889114536454),\n",
       " (np.str_('crapilicious'), 0.00026098344418575226)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Original: Why is #aussietv so white? #MKR #theblock #ImACelebrityAU #today #sunrise #studio10 #Neighbours #WonderlandTen #etc\n",
      "Original: Why is #aussietv so white? #MKR #theblock #ImACelebrityAU #today #sunrise #studio10 #Neighbours #WonderlandTen #etc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(np.str_('white'), -1.613531223947455e-05)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Original: @XochitlSuckkks a classy whore? Or more red velvet cupcakes?\n",
      "Original: @XochitlSuckkks a classy whore? Or more red velvet cupcakes?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(np.str_('whore'), 0.0053927520000426514),\n",
       " (np.str_('classy'), -0.004928811283545458),\n",
       " (np.str_('velvet'), 0.002638974555206073),\n",
       " (np.str_('red'), 0.0021224102262066575),\n",
       " (np.str_('cupcakes'), 0.0015056497165607014)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from lime.lime_text import LimeTextExplainer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "explainer = LimeTextExplainer(class_names=list(classes))\n",
    "\n",
    "def predict_proba_from_texts(texts):\n",
    "    X_tmp = np.vstack([get_glove_vector(clean_text(t)) for t in texts])\n",
    "    X_tmp_pca = pca.transform(X_tmp)\n",
    "    X_tmp_tensor = torch.tensor(X_tmp_pca, dtype=torch.float32).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        logits = model(X_tmp_tensor)\n",
    "        probs = F.softmax(logits, dim=1).cpu().numpy()\n",
    "    return probs\n",
    "\n",
    "# Pick a few examples to interpret\n",
    "for idx in range(3):\n",
    "    text = df['clean_text'].iloc[idx]\n",
    "    probs = predict_proba_from_texts([text])[0]\n",
    "    pred_class = classes[np.argmax(probs)]\n",
    "    confidence = np.max(probs)\n",
    "    \n",
    "    print(f\"\\nðŸ§© Example {idx+1}\")\n",
    "    print(\"Original Tweet:\", df[TEXT_COL].iloc[idx])\n",
    "    print(f\"Predicted: {pred_class} ({confidence:.2f} confidence)\")\n",
    "    \n",
    "    exp = explainer.explain_instance(text, predict_proba_from_texts, num_features=10)\n",
    "    display(exp.as_list())  # textual feature-weight pairs\n",
    "    \n",
    "    # Bar-plot visualization like in IEEE Figure 7\n",
    "    fig = exp.as_pyplot_figure()\n",
    "    plt.title(f\"LIME Explanation for Tweet #{idx+1} â€” Predicted: {pred_class}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"-\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
