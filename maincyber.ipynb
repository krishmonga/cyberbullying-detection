{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba183b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 ‚Äî installs (only if needed) and imports\n",
    "!pip install -q transformers datasets lime tqdm\n",
    "\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "\n",
    "\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "528a9bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 ‚Äî file paths & hyperparams (edit paths if required)\n",
    "DATA_FILE = \"cyberbullying_tweets.csv\"   # your dataset from Kaggle (andrewmvd)\n",
    "GLOVE_FILE = \"glove.6B.300d.txt\"         # make sure this file is in working directory\n",
    "\n",
    "# GloVe/token settings to reproduce ~25,000 dims\n",
    "EMBED_DIM = 300\n",
    "MAX_TOKENS_FOR_25K = 83   # 83*300 = 24,900\n",
    "PAD_TO_25K = 25000        # we will pad to exactly 25,000 dims\n",
    "\n",
    "# PCA target per paper\n",
    "PCA_N_COMPONENTS = 9000\n",
    "\n",
    "# Transformer backbone\n",
    "ROBERTA_MODEL_NAME = \"roberta-base\"\n",
    "\n",
    "# Training hyperparams (paper used fairly large batches / longer training,\n",
    "# but these are reasonable defaults; you can increase if you have GPU memory)\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 6\n",
    "LR = 2e-5\n",
    "WEIGHT_DECAY = 1e-2\n",
    "WARMUP_STEPS = 0\n",
    "\n",
    "# Cross-validation folds\n",
    "KFOLD = 5\n",
    "\n",
    "# Files to save\n",
    "PCA_MODEL_PATH = \"ipca_9000.npy\"   # we'll save sklearn PCA components and mean manually\n",
    "PCA_MEAN_PATH = \"ipca_mean.npy\"\n",
    "ROBERTA_FINE_TUNED_PATH = \"best_robertanet.pt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd7ad5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using text col: tweet_text label col: cyberbullying_type\n",
      "Classes: ['age', 'ethnicity', 'gender', 'not_cyberbullying', 'other_cyberbullying', 'religion']\n",
      "Dataset size: 47692\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 ‚Äî load dataset and preprocess text\n",
    "assert os.path.exists(DATA_FILE), f\"Dataset not found: {DATA_FILE}\"\n",
    "df = pd.read_csv(DATA_FILE)\n",
    "\n",
    "# Inspect and adapt to your CSV columns\n",
    "# Many Kaggle versions use columns like 'tweet_text' and 'cyberbullying_type' or 'label'\n",
    "# Adapt these names if your file differs.\n",
    "if \"tweet_text\" in df.columns:\n",
    "    TEXT_COL = \"tweet_text\"\n",
    "elif \"text\" in df.columns:\n",
    "    TEXT_COL = \"text\"\n",
    "elif \"tweet\" in df.columns:\n",
    "    TEXT_COL = \"tweet\"\n",
    "else:\n",
    "    # fallback: pick first object dtype column\n",
    "    TEXT_COL = df.select_dtypes(include=['object']).columns[0]\n",
    "\n",
    "if \"cyberbullying_type\" in df.columns:\n",
    "    LABEL_COL = \"cyberbullying_type\"\n",
    "elif \"label\" in df.columns:\n",
    "    LABEL_COL = \"label\"\n",
    "else:\n",
    "    LABEL_COL = df.select_dtypes(include=['int','object']).columns[1]\n",
    "\n",
    "print(\"Using text col:\", TEXT_COL, \"label col:\", LABEL_COL)\n",
    "df = df[[TEXT_COL, LABEL_COL]].dropna().reset_index(drop=True)\n",
    "\n",
    "# Basic cleaning function (paper used basic cleaning)\n",
    "import re\n",
    "def clean_text(t):\n",
    "    t = str(t).lower()\n",
    "    t = re.sub(r\"http\\S+|www\\S+\", \"\", t)\n",
    "    t = re.sub(r\"[^a-z0-9\\s@#']\", \" \", t)  # keep hashtags/mentions if helpful\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    return t\n",
    "\n",
    "df[\"clean_text\"] = df[TEXT_COL].astype(str).apply(clean_text)\n",
    "\n",
    "# label encode\n",
    "le = LabelEncoder()\n",
    "df[\"label_enc\"] = le.fit_transform(df[LABEL_COL].astype(str))\n",
    "classes = list(le.classes_)\n",
    "print(\"Classes:\", classes)\n",
    "print(\"Dataset size:\", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5c79f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading GloVe: 400000it [00:40, 9965.43it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded GloVe vectors: 400000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 4 ‚Äî load GloVe embeddings\n",
    "assert os.path.exists(GLOVE_FILE), f\"GloVe file not found: {GLOVE_FILE}\"\n",
    "embeddings_index = {}\n",
    "with open(GLOVE_FILE, 'r', encoding='utf8') as f:\n",
    "    for line in tqdm(f, desc=\"Loading GloVe\"):\n",
    "        parts = line.rstrip().split(\" \")\n",
    "        word = parts[0]\n",
    "        vec = np.asarray(parts[1:], dtype=np.float32)\n",
    "        embeddings_index[word] = vec\n",
    "print(\"Loaded GloVe vectors:\", len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ef04f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 25,000-dim GloVe vectors for each tweet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47692/47692 [00:12<00:00, 3839.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_glove shape: (47692, 25000)\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 ‚Äî produce per-tweet GloVe vectors -> target 25,000 dims\n",
    "MAX_TOKENS = MAX_TOKENS_FOR_25K  # = 83\n",
    "EMBED_DIM = 300\n",
    "\n",
    "def get_glove_flat_vector(text, max_tokens=MAX_TOKENS, pad_to=PAD_TO_25K):\n",
    "    # take token-level glove vectors (first max_tokens tokens) and flatten\n",
    "    words = text.split()[:max_tokens]\n",
    "    vecs = []\n",
    "    for w in words:\n",
    "        v = embeddings_index.get(w)\n",
    "        if v is None:\n",
    "            # try simple normalization (strip punctuation)\n",
    "            w2 = re.sub(r\"[^a-z0-9]\", \"\", w)\n",
    "            v = embeddings_index.get(w2, np.zeros(EMBED_DIM, dtype=np.float32))\n",
    "        vecs.append(v)\n",
    "    # pad tokens if fewer than max_tokens\n",
    "    while len(vecs) < max_tokens:\n",
    "        vecs.append(np.zeros(EMBED_DIM, dtype=np.float32))\n",
    "    flat = np.concatenate(vecs)  # shape = max_tokens * EMBED_DIM (24,900)\n",
    "    # pad/truncate to exact 25,000 dims\n",
    "    if flat.shape[0] < pad_to:\n",
    "        pad_len = pad_to - flat.shape[0]\n",
    "        flat = np.concatenate([flat, np.zeros(pad_len, dtype=np.float32)])\n",
    "    elif flat.shape[0] > pad_to:\n",
    "        flat = flat[:pad_to]\n",
    "    return flat\n",
    "\n",
    "# create X_glove (may take a bit of memory)\n",
    "print(\"Generating 25,000-dim GloVe vectors for each tweet...\")\n",
    "X_glove = np.vstack([get_glove_flat_vector(t) for t in tqdm(df['clean_text'])])\n",
    "y = df['label_enc'].values\n",
    "print(\"X_glove shape:\", X_glove.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2207c966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Detected system RAM: 7.65 GB\n",
      "Approx. dataset size in RAM: 4.44 GB\n",
      "‚öôÔ∏è Using IncrementalPCA (RAM-safe mode)...\n",
      "‚Üí n_components_fit=9000, batch_size=10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Partial fitting PCA:  20%|‚ñà‚ñà        | 1/5 [11:24<45:39, 684.94s/it]\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 3.54 GiB for an array with shape (19001, 25000) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 38\u001b[0m\n\u001b[0;32m     36\u001b[0m ipca \u001b[38;5;241m=\u001b[39m IncrementalPCA(n_components\u001b[38;5;241m=\u001b[39mn_comp_fit)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, n_samples, safe_batch), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPartial fitting PCA\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 38\u001b[0m     ipca\u001b[38;5;241m.\u001b[39mpartial_fit(X_glove[i:i\u001b[38;5;241m+\u001b[39msafe_batch])\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# transform all data\u001b[39;00m\n\u001b[0;32m     41\u001b[0m X_pca_parts \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mq:\\Anaconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mq:\\Anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_incremental_pca.py:354\u001b[0m, in \u001b[0;36mIncrementalPCA.partial_fit\u001b[1;34m(self, X, y, check_input)\u001b[0m\n\u001b[0;32m    350\u001b[0m     \u001b[38;5;66;03m# Build matrix of combined previous basis and new data\u001b[39;00m\n\u001b[0;32m    351\u001b[0m     mean_correction \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\n\u001b[0;32m    352\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_samples_seen_ \u001b[38;5;241m/\u001b[39m n_total_samples) \u001b[38;5;241m*\u001b[39m n_samples\n\u001b[0;32m    353\u001b[0m     ) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_ \u001b[38;5;241m-\u001b[39m col_batch_mean)\n\u001b[1;32m--> 354\u001b[0m     X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack(\n\u001b[0;32m    355\u001b[0m         (\n\u001b[0;32m    356\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msingular_values_\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomponents_,\n\u001b[0;32m    357\u001b[0m             X,\n\u001b[0;32m    358\u001b[0m             mean_correction,\n\u001b[0;32m    359\u001b[0m         )\n\u001b[0;32m    360\u001b[0m     )\n\u001b[0;32m    362\u001b[0m U, S, Vt \u001b[38;5;241m=\u001b[39m linalg\u001b[38;5;241m.\u001b[39msvd(X, full_matrices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, check_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    363\u001b[0m U, Vt \u001b[38;5;241m=\u001b[39m svd_flip(U, Vt, u_based_decision\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mq:\\Anaconda3\\Lib\\site-packages\\numpy\\_core\\shape_base.py:291\u001b[0m, in \u001b[0;36mvstack\u001b[1;34m(tup, dtype, casting)\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arrs, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    290\u001b[0m     arrs \u001b[38;5;241m=\u001b[39m (arrs,)\n\u001b[1;32m--> 291\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _nx\u001b[38;5;241m.\u001b[39mconcatenate(arrs, \u001b[38;5;241m0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype, casting\u001b[38;5;241m=\u001b[39mcasting)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 3.54 GiB for an array with shape (19001, 25000) and data type float64"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Cell 6 ‚Äî Adaptive IncrementalPCA (auto RAM-safe)\n",
    "import psutil\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "import gc\n",
    "\n",
    "# --- Detect system memory ---\n",
    "total_ram_gb = psutil.virtual_memory().total / (1024**3)\n",
    "print(f\"üß† Detected system RAM: {total_ram_gb:.2f} GB\")\n",
    "\n",
    "# --- Configuration ---\n",
    "target_components = PCA_N_COMPONENTS  # 9000 per paper\n",
    "n_samples, n_features = X_glove.shape\n",
    "\n",
    "# Estimate memory for one full PCA matrix (float32)\n",
    "approx_mem_gb = (n_samples * n_features * 4) / (1024**3)\n",
    "print(f\"Approx. dataset size in RAM: {approx_mem_gb:.2f} GB\")\n",
    "\n",
    "# --- Decide mode ---\n",
    "if total_ram_gb > approx_mem_gb * 2.5:\n",
    "    # plenty of RAM ‚Äî use full PCA\n",
    "    print(f\"‚úÖ Using full PCA with {target_components} components...\")\n",
    "    pca = PCA(n_components=target_components, random_state=42)\n",
    "    X_pca = pca.fit_transform(X_glove)\n",
    "    components, mean = pca.components_, pca.mean_\n",
    "else:\n",
    "    # limited RAM ‚Äî use IncrementalPCA safely\n",
    "    print(f\"‚öôÔ∏è Using IncrementalPCA (RAM-safe mode)...\")\n",
    "\n",
    "    # pick a batch size to be at least equal to n_components\n",
    "    safe_batch = max(min(10000, n_samples), target_components)\n",
    "    safe_batch = min(safe_batch, n_samples)\n",
    "    n_comp_fit = min(target_components, safe_batch)\n",
    "\n",
    "    print(f\"‚Üí n_components_fit={n_comp_fit}, batch_size={safe_batch}\")\n",
    "\n",
    "    ipca = IncrementalPCA(n_components=n_comp_fit)\n",
    "    for i in tqdm(range(0, n_samples, safe_batch), desc=\"Partial fitting PCA\"):\n",
    "        ipca.partial_fit(X_glove[i:i+safe_batch])\n",
    "\n",
    "    # transform all data\n",
    "    X_pca_parts = []\n",
    "    for i in tqdm(range(0, n_samples, safe_batch), desc=\"Transforming batches\"):\n",
    "        X_pca_parts.append(ipca.transform(X_glove[i:i+safe_batch]))\n",
    "        gc.collect()\n",
    "    X_pca = np.vstack(X_pca_parts).astype(np.float32)\n",
    "    del X_pca_parts\n",
    "    gc.collect()\n",
    "\n",
    "    components, mean = ipca.components_, ipca.mean_\n",
    "\n",
    "# --- Save PCA results ---\n",
    "np.save(\"ipca_components.npy\", components)\n",
    "np.save(\"ipca_mean.npy\", mean)\n",
    "print(\"‚úÖ PCA complete. X_pca shape:\", X_pca.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6166abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7 ‚Äî Tokenizer + RoBERTa backbone + classifier that concatenates PCA features\n",
    "tokenizer = AutoTokenizer.from_pretrained(ROBERTA_MODEL_NAME)\n",
    "roberta = AutoModel.from_pretrained(ROBERTA_MODEL_NAME).to(DEVICE)\n",
    "roberta.config.output_hidden_states = False\n",
    "\n",
    "# Classifier that concatenates RoBERTa pooled output (768) with PCA features (9000)\n",
    "class RobertaPCAClassifier(nn.Module):\n",
    "    def __init__(self, pca_dim, hidden_dim=1024, num_classes=len(classes), roberta_model=roberta):\n",
    "        super().__init__()\n",
    "        self.roberta = roberta_model  # pretrained backbone\n",
    "        # freeze/unfreeze policy: paper fine-tunes ‚Äî we will fine-tune full model\n",
    "        # If memory/time limited, you can freeze `self.roberta` layers and only train head.\n",
    "        self.pca_proj = nn.Linear(pca_dim, hidden_dim)  # project PCA features\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.roberta.config.hidden_size + hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "    def forward(self, input_ids=None, attention_mask=None, pca_feats=None):\n",
    "        # roberta forward\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "        # pooled representation: use mean pooling of last hidden state across tokens\n",
    "        last_hidden = outputs.last_hidden_state  # (B, T, H)\n",
    "        pooled = last_hidden.mean(dim=1)  # (B, H)\n",
    "        pca_proj = F.relu(self.pca_proj(pca_feats))  # (B, hidden_dim)\n",
    "        fused = torch.cat([pooled, pca_proj], dim=1)\n",
    "        logits = self.classifier(fused)\n",
    "        return logits\n",
    "\n",
    "# initialize model\n",
    "pca_dim = PCA_N_COMPONENTS\n",
    "model = RobertaPCAClassifier(pca_dim=pca_dim).to(DEVICE)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31991ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Cell 7A ‚Äî Machine Learning baselines on PCA features (RF, SVM, NB, KNN)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "print(\"Training classical ML models on PCA (9,000) features...\")\n",
    "\n",
    "# scale features for distance-based models\n",
    "scaler = StandardScaler()\n",
    "X_pca_scaled = scaler.fit_transform(X_pca)\n",
    "\n",
    "# 80/20 split for baseline testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_ml, X_test_ml, y_train_ml, y_test_ml = train_test_split(\n",
    "    X_pca_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "ml_models = {\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    \"LinearSVM\": LinearSVC(C=1.0, max_iter=5000, random_state=42),\n",
    "    \"NaiveBayes\": GaussianNB(),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5)\n",
    "}\n",
    "\n",
    "ml_results = []\n",
    "for name, clf in ml_models.items():\n",
    "    print(f\"‚ñ∂ Training {name} ...\")\n",
    "    clf.fit(X_train_ml, y_train_ml)\n",
    "    preds = clf.predict(X_test_ml)\n",
    "    acc = accuracy_score(y_test_ml, preds)\n",
    "    prec = precision_score(y_test_ml, preds, average='macro', zero_division=0)\n",
    "    rec = recall_score(y_test_ml, preds, average='macro', zero_division=0)\n",
    "    f1 = f1_score(y_test_ml, preds, average='macro', zero_division=0)\n",
    "    ml_results.append([name, acc, prec, rec, f1])\n",
    "    print(f\"{name}: Acc={acc:.4f} | F1={f1:.4f}\")\n",
    "\n",
    "ml_df = pd.DataFrame(ml_results, columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1'])\n",
    "display(ml_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57651ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Cell 7B ‚Äî Deep Learning baselines (CNN, BiLSTM, ConvLSTM)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# reshape GloVe features to (samples, seq_len, embed_dim) = (N, 30, 300)\n",
    "SEQ_LEN, EMBED_DIM = 30, 300\n",
    "X_glove_reshaped = X_glove[:, :SEQ_LEN*EMBED_DIM].reshape(-1, SEQ_LEN, EMBED_DIM)\n",
    "\n",
    "X_train_dl, X_test_dl, y_train_dl, y_test_dl = train_test_split(\n",
    "    X_glove_reshaped, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "train_ds = TensorDataset(torch.tensor(X_train_dl, dtype=torch.float32), torch.tensor(y_train_dl))\n",
    "test_ds  = TensorDataset(torch.tensor(X_test_dl,  dtype=torch.float32), torch.tensor(y_test_dl))\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=64, shuffle=False)\n",
    "\n",
    "# Simple CNN baseline\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=len(np.unique(y))):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(EMBED_DIM, 128, 3, padding=1)\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0,2,1)  # (B, 300, 30)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x).squeeze(-1)\n",
    "        return self.fc(x)\n",
    "\n",
    "# BiLSTM baseline\n",
    "class SimpleBiLSTM(nn.Module):\n",
    "    def __init__(self, hidden=128, num_classes=len(np.unique(y))):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(EMBED_DIM, hidden, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden*2, num_classes)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        return self.fc(out[:,-1,:])\n",
    "\n",
    "# ConvLSTM hybrid baseline\n",
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self, hidden=128, num_classes=len(np.unique(y))):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(EMBED_DIM, 128, 3, padding=1)\n",
    "        self.lstm = nn.LSTM(128, hidden, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0,2,1)\n",
    "        x = F.relu(self.conv(x)).permute(0,2,1)\n",
    "        out, _ = self.lstm(x)\n",
    "        return self.fc(out[:,-1,:])\n",
    "\n",
    "def train_and_eval(model, loader_train, loader_test, epochs=5):\n",
    "    model = model.to(DEVICE)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    for ep in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in loader_train:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            opt.zero_grad()\n",
    "            loss = loss_fn(model(xb), yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader_test:\n",
    "            out = model(xb.to(DEVICE))\n",
    "            preds += torch.argmax(out, dim=1).cpu().tolist()\n",
    "            trues += yb.tolist()\n",
    "    return (\n",
    "        accuracy_score(trues, preds),\n",
    "        precision_score(trues, preds, average='macro', zero_division=0),\n",
    "        recall_score(trues, preds, average='macro', zero_division=0),\n",
    "        f1_score(trues, preds, average='macro', zero_division=0),\n",
    "    )\n",
    "\n",
    "# Train each DL model\n",
    "cnn_metrics = train_and_eval(SimpleCNN(), train_loader, test_loader)\n",
    "bilstm_metrics = train_and_eval(SimpleBiLSTM(), train_loader, test_loader)\n",
    "convlstm_metrics = train_and_eval(ConvLSTM(), train_loader, test_loader)\n",
    "\n",
    "dl_df = pd.DataFrame([\n",
    "    ['CNN', *cnn_metrics],\n",
    "    ['BiLSTM', *bilstm_metrics],\n",
    "    ['ConvLSTM', *convlstm_metrics]\n",
    "], columns=['Model','Accuracy','Precision','Recall','F1'])\n",
    "\n",
    "print(\"‚úÖ Deep Learning baselines complete:\")\n",
    "display(dl_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ff4afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7C ‚Äî BERT baseline (fine-tune bert-base-uncased, 5-fold CV)\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "BERT_MODEL_NAME = \"bert-base-uncased\"\n",
    "print(\"Preparing BERT baseline:\", BERT_MODEL_NAME)\n",
    "\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
    "bert_backbone = AutoModel.from_pretrained(BERT_MODEL_NAME).to(DEVICE)\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=len(classes), backbone=bert_backbone, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.backbone.config.hidden_size, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "    def forward(self, input_ids=None, attention_mask=None, pca_feats=None):\n",
    "        # ignore pca_feats (BERT baseline uses text only)\n",
    "        outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "        pooled = outputs.last_hidden_state.mean(dim=1)\n",
    "        logits = self.fc(pooled)\n",
    "        return logits\n",
    "\n",
    "# 5-fold CV for BERT (text-only)\n",
    "skf = StratifiedKFold(n_splits=KFOLD, shuffle=True, random_state=SEED)\n",
    "bert_fold_metrics = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(df['clean_text'], y), 1):\n",
    "    print(f\"\\n>>> BERT Fold {fold}/{KFOLD}\")\n",
    "    train_texts = df.loc[train_idx, 'clean_text'].tolist()\n",
    "    val_texts = df.loc[val_idx, 'clean_text'].tolist()\n",
    "    y_train = y[train_idx]\n",
    "    y_val = y[val_idx]\n",
    "    # datasets & loaders\n",
    "    train_ds = RobertaPCADataset(train_texts, np.zeros((len(train_texts), pca_dim)), y_train, tokenizer_bert)  # pca_feats ignored\n",
    "    val_ds   = RobertaPCADataset(val_texts,   np.zeros((len(val_texts),   pca_dim)), y_val,   tokenizer_bert)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    model_bert = BertClassifier(num_classes=len(classes)).to(DEVICE)\n",
    "    optimizer = AdamW(model_bert.parameters(), lr=2e-5, weight_decay=1e-2)\n",
    "    total_steps = len(train_loader)*NUM_EPOCHS\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps=total_steps)\n",
    "\n",
    "    best_val_f1 = 0.0\n",
    "    for epoch in range(1, NUM_EPOCHS+1):\n",
    "        _ = train_one_epoch(model_bert, train_loader, optimizer, scheduler)\n",
    "        stats = evaluate_model(model_bert, val_loader)\n",
    "        print(f\"BERT Fold{fold} E{epoch} ‚Äî Val F1: {stats['f1']:.4f} Acc: {stats['accuracy']:.4f}\")\n",
    "        if stats['f1'] > best_val_f1:\n",
    "            best_val_f1 = stats['f1']\n",
    "            torch.save(model_bert.state_dict(), f\"best_bert_fold{fold}.pt\")\n",
    "    model_bert.load_state_dict(torch.load(f\"best_bert_fold{fold}.pt\"))\n",
    "    final_stats = evaluate_model(model_bert, val_loader)\n",
    "    print(\"BERT Fold final:\", final_stats)\n",
    "    bert_fold_metrics.append(final_stats)\n",
    "\n",
    "# Aggregate BERT results\n",
    "bert_acc = np.mean([m['accuracy'] for m in bert_fold_metrics])\n",
    "bert_prec = np.mean([m['precision'] for m in bert_fold_metrics])\n",
    "bert_rec = np.mean([m['recall'] for m in bert_fold_metrics])\n",
    "bert_f1 = np.mean([m['f1'] for m in bert_fold_metrics])\n",
    "print(\"\\nBERT 5-fold mean ‚Äî Acc: {:.4f} Prec: {:.4f} Rec: {:.4f} F1: {:.4f}\".format(bert_acc, bert_prec, bert_rec, bert_f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262113d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7D ‚Äî RoBERTa-only baseline (no PCA fusion), 5-fold CV\n",
    "print(\"Preparing RoBERTa-only baseline (text-only)\")\n",
    "\n",
    "tokenizer_roberta_text = AutoTokenizer.from_pretrained(ROBERTA_MODEL_NAME)\n",
    "roberta_backbone_text = AutoModel.from_pretrained(ROBERTA_MODEL_NAME).to(DEVICE)\n",
    "\n",
    "class RobertaTextClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=len(classes), backbone=roberta_backbone_text, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.backbone.config.hidden_size, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "    def forward(self, input_ids=None, attention_mask=None, pca_feats=None):\n",
    "        outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "        pooled = outputs.last_hidden_state.mean(dim=1)\n",
    "        logits = self.fc(pooled)\n",
    "        return logits\n",
    "\n",
    "roberta_text_fold_metrics = []\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(df['clean_text'], y), 1):\n",
    "    print(f\"\\n>>> RoBERTa-text Fold {fold}/{KFOLD}\")\n",
    "    train_texts = df.loc[train_idx, 'clean_text'].tolist()\n",
    "    val_texts = df.loc[val_idx, 'clean_text'].tolist()\n",
    "    y_train = y[train_idx]\n",
    "    y_val = y[val_idx]\n",
    "\n",
    "    train_ds = RobertaPCADataset(train_texts, np.zeros((len(train_texts), pca_dim)), y_train, tokenizer_roberta_text)\n",
    "    val_ds   = RobertaPCADataset(val_texts,   np.zeros((len(val_texts),   pca_dim)), y_val,   tokenizer_roberta_text)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    model_rb_text = RobertaTextClassifier(num_classes=len(classes)).to(DEVICE)\n",
    "    optimizer = AdamW(model_rb_text.parameters(), lr=2e-5, weight_decay=1e-2)\n",
    "    total_steps = len(train_loader)*NUM_EPOCHS\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps=total_steps)\n",
    "\n",
    "    best_val_f1 = 0.0\n",
    "    for epoch in range(1, NUM_EPOCHS+1):\n",
    "        _ = train_one_epoch(model_rb_text, train_loader, optimizer, scheduler)\n",
    "        stats = evaluate_model(model_rb_text, val_loader)\n",
    "        print(f\"RoBERTa-text Fold{fold} E{epoch} ‚Äî Val F1: {stats['f1']:.4f} Acc: {stats['accuracy']:.4f}\")\n",
    "        if stats['f1'] > best_val_f1:\n",
    "            best_val_f1 = stats['f1']\n",
    "            torch.save(model_rb_text.state_dict(), f\"best_roberta_text_fold{fold}.pt\")\n",
    "    model_rb_text.load_state_dict(torch.load(f\"best_roberta_text_fold{fold}.pt\"))\n",
    "    final_stats = evaluate_model(model_rb_text, val_loader)\n",
    "    print(\"RoBERTa-text Fold final:\", final_stats)\n",
    "    roberta_text_fold_metrics.append(final_stats)\n",
    "\n",
    "# Aggregate RoBERTa-only results\n",
    "roberta_text_acc = np.mean([m['accuracy'] for m in roberta_text_fold_metrics])\n",
    "roberta_text_prec = np.mean([m['precision'] for m in roberta_text_fold_metrics])\n",
    "roberta_text_rec = np.mean([m['recall'] for m in roberta_text_fold_metrics])\n",
    "roberta_text_f1 = np.mean([m['f1'] for m in roberta_text_fold_metrics])\n",
    "print(\"\\nRoBERTa-text 5-fold mean ‚Äî Acc: {:.4f} Prec: {:.4f} Rec: {:.4f} F1: {:.4f}\".format(\n",
    "    roberta_text_acc, roberta_text_prec, roberta_text_rec, roberta_text_f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09046546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8 ‚Äî dataset + dataloader helpers\n",
    "class RobertaPCADataset(Dataset):\n",
    "    def __init__(self, texts, pca_feats, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.pca_feats = pca_feats.astype(np.float32)\n",
    "        self.labels = labels.astype(np.int64)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        enc = self.tokenizer(text,\n",
    "                              truncation=True,\n",
    "                              max_length=self.max_length,\n",
    "                              padding='max_length',\n",
    "                              return_tensors='pt')\n",
    "        item = {\n",
    "            'input_ids': enc['input_ids'].squeeze(0),\n",
    "            'attention_mask': enc['attention_mask'].squeeze(0),\n",
    "            'pca_feats': torch.from_numpy(self.pca_feats[idx]),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "        return item\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([b['input_ids'] for b in batch])\n",
    "    attention_mask = torch.stack([b['attention_mask'] for b in batch])\n",
    "    pca_feats = torch.stack([b['pca_feats'] for b in batch])\n",
    "    labels = torch.stack([b['labels'] for b in batch])\n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'pca_feats': pca_feats, 'labels': labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d5cc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9 ‚Äî training & evaluation functions\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, scheduler=None, clip_grad=1.0):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    pbar = tqdm(dataloader, desc=\"Train batch\")\n",
    "    for batch in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        pca_feats = batch['pca_feats'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "        logits = model(input_ids=input_ids, attention_mask=attention_mask, pca_feats=pca_feats)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        total_loss += loss.item() * input_ids.size(0)\n",
    "    return total_loss / len(dataloader.dataset)\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            pca_feats = batch['pca_feats'].to(DEVICE)\n",
    "            labels = batch['labels'].cpu().numpy()\n",
    "\n",
    "            logits = model(input_ids=input_ids, attention_mask=attention_mask, pca_feats=pca_feats)\n",
    "            batch_preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            preds.extend(batch_preds.tolist())\n",
    "            trues.extend(labels.tolist())\n",
    "    acc = accuracy_score(trues, preds)\n",
    "    prec = precision_score(trues, preds, average='macro', zero_division=0)\n",
    "    rec = recall_score(trues, preds, average='macro', zero_division=0)\n",
    "    f1 = f1_score(trues, preds, average='macro', zero_division=0)\n",
    "    return {\"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1\": f1, \"preds\": preds, \"trues\": trues}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b30bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10 ‚Äî 5-fold cross validation running fine-tuning and reporting\n",
    "skf = StratifiedKFold(n_splits=KFOLD, shuffle=True, random_state=SEED)\n",
    "\n",
    "fold_metrics = []\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(df['clean_text'], y), 1):\n",
    "    print(f\"\\n===== Fold {fold}/{KFOLD} =====\")\n",
    "\n",
    "    # create datasets\n",
    "    train_texts = df.loc[train_idx, 'clean_text'].tolist()\n",
    "    val_texts = df.loc[val_idx, 'clean_text'].tolist()\n",
    "    X_train_pca = X_pca[train_idx]\n",
    "    X_val_pca = X_pca[val_idx]\n",
    "    y_train = y[train_idx]\n",
    "    y_val = y[val_idx]\n",
    "\n",
    "    train_ds = RobertaPCADataset(train_texts, X_train_pca, y_train, tokenizer)\n",
    "    val_ds = RobertaPCADataset(val_texts, X_val_pca, y_val, tokenizer)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # initialize a new model instance for each fold (fresh weights)\n",
    "    model = RobertaPCAClassifier(pca_dim=pca_dim).to(DEVICE)\n",
    "\n",
    "    # optimizer & scheduler (fine-tune whole model)\n",
    "    optimizer = AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    total_steps = len(train_loader) * NUM_EPOCHS\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps=total_steps)\n",
    "\n",
    "    best_val_f1 = 0.0\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, scheduler)\n",
    "        val_stats = evaluate_model(model, val_loader)\n",
    "        print(f\"Fold {fold} Epoch {epoch}/{NUM_EPOCHS} ‚Äî TrainLoss: {train_loss:.4f} Val F1: {val_stats['f1']:.4f} Val Acc: {val_stats['accuracy']:.4f}\")\n",
    "        # save best model for fold\n",
    "        if val_stats['f1'] > best_val_f1:\n",
    "            best_val_f1 = val_stats['f1']\n",
    "            torch.save(model.state_dict(), f\"best_robertanet_fold{fold}.pt\")\n",
    "    # load best and evaluate (on validation)\n",
    "    model.load_state_dict(torch.load(f\"best_robertanet_fold{fold}.pt\"))\n",
    "    val_stats_final = evaluate_model(model, val_loader)\n",
    "    print(\"Fold final metrics:\", val_stats_final)\n",
    "    fold_metrics.append(val_stats_final)\n",
    "\n",
    "# aggregate and print mean ¬± std\n",
    "accs = [m['accuracy'] for m in fold_metrics]\n",
    "precs = [m['precision'] for m in fold_metrics]\n",
    "recs = [m['recall'] for m in fold_metrics]\n",
    "f1s = [m['f1'] for m in fold_metrics]\n",
    "print(\"\\n=== 5-Fold CV Results ===\")\n",
    "print(f\"Accuracy : {np.mean(accs):.4f} ¬± {np.std(accs):.4f}\")\n",
    "print(f\"Precision: {np.mean(precs):.4f} ¬± {np.std(precs):.4f}\")\n",
    "print(f\"Recall   : {np.mean(recs):.4f} ¬± {np.std(recs):.4f}\")\n",
    "print(f\"F1-score : {np.mean(f1s):.4f} ¬± {np.std(f1s):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600db2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11 ‚Äî if you have separate test set, evaluate final saved best model, otherwise\n",
    "# train on full dataset and save final model (paper reports final test; here we show training on full data)\n",
    "full_ds = RobertaPCADataset(df['clean_text'].tolist(), X_pca, y, tokenizer)\n",
    "full_loader = DataLoader(full_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Train final model on full dataset (if you want) ‚Äî careful, this is optional and expensive\n",
    "final_model = RobertaPCAClassifier(pca_dim=pca_dim).to(DEVICE)\n",
    "optimizer = AdamW(final_model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps=len(full_loader)*NUM_EPOCHS)\n",
    "\n",
    "best_f1 = 0.0\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    train_loss = train_one_epoch(final_model, full_loader, optimizer, scheduler)\n",
    "    # no validation here unless you have a held-out test set\n",
    "    print(f\"Epoch {epoch}/{NUM_EPOCHS} | Train Loss: {train_loss:.4f}\")\n",
    "    # optionally save each epoch\n",
    "    if (epoch % 2) == 0:\n",
    "        torch.save(final_model.state_dict(), f\"final_robertanet_epoch{epoch}.pt\")\n",
    "\n",
    "# Save final model\n",
    "torch.save(final_model.state_dict(), ROBERTA_FINE_TUNED_PATH)\n",
    "print(\"Final model saved to\", ROBERTA_FINE_TUNED_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e54b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12 ‚Äî LIME: define predict_proba wrapper that accepts raw texts and returns class probabilities\n",
    "# Load PCA components & mean (we saved earlier)\n",
    "ipca_components = np.load(\"ipca_components.npy\")\n",
    "ipca_mean = np.load(\"ipca_mean.npy\")\n",
    "\n",
    "# recreate IncrementalPCA object for transform (we'll use sklearn PCA transform formula)\n",
    "def glove_text_to_pca_vector(text):\n",
    "    # replicate get_glove_flat_vector then center and project: x_pca = (x - mean) dot components_.T\n",
    "    xflat = get_glove_flat_vector(clean_text(text))\n",
    "    x_centered = xflat - ipca_mean\n",
    "    # components shape (n_components, n_features) => project:\n",
    "    x_pca = np.dot(x_centered, ipca_components.T)  # shape (n_components,)\n",
    "    return x_pca.astype(np.float32)\n",
    "\n",
    "# load the final model (if not loaded)\n",
    "model = RobertaPCAClassifier(pca_dim=pca_dim).to(DEVICE)\n",
    "if os.path.exists(ROBERTA_FINE_TUNED_PATH):\n",
    "    model.load_state_dict(torch.load(ROBERTA_FINE_TUNED_PATH, map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "def predict_proba_from_texts(texts):\n",
    "    # texts: list of strings -> returns array (n_texts, n_classes) of probabilities\n",
    "    batch_inputs = tokenizer(texts, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "    input_ids = batch_inputs['input_ids'].to(DEVICE)\n",
    "    attention_mask = batch_inputs['attention_mask'].to(DEVICE)\n",
    "    # compute PCA features for each text (on CPU then move to device)\n",
    "    pca_feats = np.vstack([glove_text_to_pca_vector(t) for t in texts])\n",
    "    pca_feats = torch.from_numpy(pca_feats).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids=input_ids, attention_mask=attention_mask, pca_feats=pca_feats)\n",
    "        probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "    return probs\n",
    "\n",
    "# LIME explainer\n",
    "explainer = LimeTextExplainer(class_names=classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8792928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13 ‚Äî generate LIME explanation for sample tweet(s)\n",
    "examples = df['clean_text'].sample(3, random_state=SEED).tolist()\n",
    "\n",
    "for i, text in enumerate(examples):\n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    print(text)\n",
    "    probs = predict_proba_from_texts([text])[0]\n",
    "    pred_label = classes[np.argmax(probs)]\n",
    "    print(\"Predicted:\", pred_label, \"Probs:\", np.round(probs, 3))\n",
    "\n",
    "    exp = explainer.explain_instance(text, predict_proba_from_texts, num_features=10)\n",
    "    print(\"LIME explanation (word, weight):\")\n",
    "    print(exp.as_list())\n",
    "    # optional: visual\n",
    "    fig = exp.as_pyplot_figure()\n",
    "    fig.suptitle(f\"LIME ‚Äî predicted: {pred_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5a02ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14 (UPDATED) ‚Äî Aggregate ML, DL, BERT, RoBERTa (text), and Proposed RoBERTa+PCA results\n",
    "\n",
    "# Prepare transformer rows\n",
    "try:\n",
    "    bert_row = ['BERT (text-only)', bert_acc, bert_prec, bert_rec, bert_f1]\n",
    "except NameError:\n",
    "    bert_row = ['BERT (text-only)', 0,0,0,0]\n",
    "\n",
    "try:\n",
    "    roberta_row = ['RoBERTa (text-only)', roberta_text_acc, roberta_text_prec, roberta_text_rec, roberta_text_f1]\n",
    "except NameError:\n",
    "    roberta_row = ['RoBERTa (text-only)', 0,0,0,0]\n",
    "\n",
    "try:\n",
    "    roberta_pca_row = ['RoBERTa + PCA-GloVe', np.mean([m['accuracy'] for m in fold_metrics]),\n",
    "                       np.mean([m['precision'] for m in fold_metrics]),\n",
    "                       np.mean([m['recall'] for m in fold_metrics]),\n",
    "                       np.mean([m['f1'] for m in fold_metrics])]\n",
    "except Exception:\n",
    "    roberta_pca_row = ['RoBERTa + PCA-GloVe', 0,0,0,0]\n",
    "\n",
    "transformer_df = pd.DataFrame([bert_row, roberta_row, roberta_pca_row],\n",
    "                              columns=['Model','Accuracy','Precision','Recall','F1'])\n",
    "\n",
    "# Combine all\n",
    "all_results = pd.concat([ml_df, dl_df, transformer_df], ignore_index=True)\n",
    "all_results = all_results.sort_values(by='F1', ascending=False).reset_index(drop=True)\n",
    "print(\"üìä Final Performance Comparison (Paper-style Table):\")\n",
    "display(all_results.style.set_properties(**{'text-align': 'center'}))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
